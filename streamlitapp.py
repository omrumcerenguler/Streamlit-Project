
from urllib.parse import quote_plus
import numpy as np
import pandas as pd
import streamlit as st
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import io
from sqlalchemy import create_engine, text
import datetime

# Prophet/cmdstanpy logging seviyesini dÃ¼ÅŸÃ¼r (terminal spam'ini engelle)
import logging
try:
    logging.getLogger("cmdstanpy").setLevel(logging.CRITICAL)
    logging.getLogger("prophet").setLevel(logging.CRITICAL)
except Exception:
    pass

# ---- CSV fallback (yerel test iÃ§in) ----
CSV_CFG = getattr(st, "secrets", {}).get("csv", {}) if hasattr(st, "secrets") else {}
CSV_DIR = CSV_CFG.get("dir", ".")  # default: current folder
CSV_ENABLE = bool(CSV_CFG.get("enable", False))

@st.cache_data(show_spinner=False)
def _csv_path(name: str) -> str:
    import os
    # beklenen dosya adlarÄ±: woshit.csv, woshitattributes.csv, wosauthor.csv, yoksisbirim.csv, wosaddress.csv, cuauthor.csv, cuauthorrid.csv
    return os.path.join(CSV_DIR, name)

@st.cache_data(show_spinner=False)
def _csv_exists(name: str) -> bool:
    import os
    return False if not CSV_ENABLE else os.path.exists(_csv_path(name))

@st.cache_data(show_spinner=False)
def _csv_load(name: str) -> pd.DataFrame:
    try:
        p = _csv_path(name)
        return pd.read_csv(p)
    except Exception:
        return pd.DataFrame()

# Opsiyonel: KÃ¼meleme iÃ§in scikit-learn
try:
    from sklearn.cluster import KMeans  # type: ignore
    from sklearn.preprocessing import StandardScaler  # type: ignore
    SKLEARN_OK = True
except Exception:
    KMeans = None  # type: ignore
    StandardScaler = None  # type: ignore
    SKLEARN_OK = False

# Prophet opsiyonel (varsa kullanÄ±lacak) â€” sembol import etmeyelim ki linter ÅŸikayet etmesin
try:
    import importlib
    importlib.import_module("prophet")
    PROPHET_OK = True
except Exception:
    PROPHET_OK = False

st.set_page_config(page_title="Ãœniversite Analizi (DB + CSV)", layout="wide")
st.title("ğŸ“ Ãœniversite Analizi â€” SQL Server / CSV Hibrit")

# Tek tÄ±kta Ã§alÄ±ÅŸ modu: yÄ±llÄ±k seri Ã¼retimi iÃ§in UI'yÄ± gizle ve otomatik Ã§alÄ±ÅŸtÄ±r
ONE_CLICK = False

# GeliÅŸtirici modu: son kullanÄ±cÄ±dan gizli diagnostik panelleri gÃ¶stermeyin
DEBUG_MODE = bool(getattr(st, "secrets", {}).get("debug_mode", False))

# ----------------- YardÄ±mcÄ±lar -----------------


@st.cache_data(show_spinner=False)
def read_csv(uploaded, sep=",", decimal="."):
    return pd.read_csv(uploaded, sep=sep, decimal=decimal)

# --- YÄ±l aralÄ±ÄŸÄ± keÅŸif yardÄ±mcÄ±larÄ± ---


@st.cache_data(show_spinner=False)
def db_year_bounds() -> tuple[int, int]:
    """VeritabanÄ±ndan mevcut en eski ve en yeni yÄ±lÄ± getirir (WOSHit.SourcePublishYear)."""
    cur_year = datetime.date.today().year
    try:
        df = sql_to_df(
            "SELECT MIN(SourcePublishYear) AS MinY, MAX(SourcePublishYear) AS MaxY "
            "FROM dbo.WOSHit WHERE SourcePublishYear IS NOT NULL"
        )
        miny = int(df["MinY"].iloc[0]) if not df.empty and pd.notna(df["MinY"].iloc[0]) else cur_year
        maxy = int(df["MaxY"].iloc[0]) if not df.empty and pd.notna(df["MaxY"].iloc[0]) else cur_year
    except Exception:
        miny, maxy = cur_year, cur_year
    maxy = min(maxy, cur_year)
    return miny, maxy

def csv_year_bounds(df: pd.DataFrame | None) -> tuple[int, int]:
    """CSV iÃ§indeki yÄ±l kolonunu bulup min/max dÃ¶ndÃ¼rÃ¼r."""
    cur_year=datetime.date.today().year
    if df is None or df.empty:
        return cur_year, cur_year
    candidates=["Yil", "YIL", "yil", "year", "Year",
        "SOURCEPUBLISHYEAR", "SourcePublishYear", "sourcepublishyear"]
    year_col=None
    for c in df.columns:
        if str(c) in candidates:
            year_col=c
            break
    if year_col is None:
        # normalize etmeyi dene
        df_tmp=normalize_columns(df.copy(), "alan_yillik")
        if "Yil" in df_tmp.columns:
            year_col="Yil"
    if year_col is None:
        return cur_year, cur_year
    s=pd.to_numeric(df[year_col], errors="coerce").dropna()
    if s.empty:
        return cur_year, cur_year
    miny, maxy=int(s.min()), int(s.max())
    maxy=min(maxy, cur_year)
    return miny, maxy

# --- AtÄ±f kolonu var mÄ± kontrolÃ¼ ve otomatik seÃ§im ---
@st.cache_data(show_spinner=False)
def db_has_column(schema: str, table: str, column: str) -> bool:
    try:
        q=(
            "SELECT 1 FROM INFORMATION_SCHEMA.COLUMNS "
            f"WHERE TABLE_SCHEMA='{_q(schema)}' AND TABLE_NAME='{_q(table)}' "
            f"AND COLUMN_NAME='{_q(column)}'"
        )
        df=sql_to_df(q, tag=f"colchk:{schema}.{table}.{column}")
        return not df.empty
    except Exception:
        return False

# --- Table existence helper ---
@st.cache_data(show_spinner=False)
def db_has_table(schema: str, table: str) -> bool:
    try:
        q = (
            "SELECT 1 FROM INFORMATION_SCHEMA.TABLES "
            f"WHERE TABLE_SCHEMA='{_q(schema)}' AND TABLE_NAME='{_q(table)}'"
        )
        df = sql_to_df(q, tag=f"tblchk:{schema}.{table}")
        return not df.empty
    except Exception:
        return False

@st.cache_data(show_spinner=False)
def detect_citation_column() -> str | None:
    # Common possibilities we saw in different dumps
    for cand in ("TimesCited", "CitiationCount", "times_cited"):
        if db_has_column("dbo", "WOSHit", cand):
            return cand
    return None

# --- Auto-detection helper for CUAUTHOR columns ---
@st.cache_data(show_spinner=False)
def detect_cuauthor_mapping() -> tuple[str | None, str | None]:
    """Try to detect CUAUTHOR table and its Hit/Author id column names."""
    schema = ORG2_SCHEMA
    tbl = CUAUTHOR_TBL
    if not db_has_table(schema, tbl):
        return None, None
    # Common candidates
    hit_cands = ["HitId", "hit_id", "WosHitId", "HITID"]
    auth_cands = ["AuthorId", "author_id", "AUTHORID"]
    found_hit = None
    found_auth = None
    for c in hit_cands:
        if db_has_column(schema, tbl, c):
            found_hit = c
            break
    for c in auth_cands:
        if db_has_column(schema, tbl, c):
            found_auth = c
            break
    return found_hit, found_auth

def build_engine():
    """SQLAlchemy engine oluÅŸturur (pyodbc + secrets.toml).
    - .streamlit/secrets.toml iÃ§inde [db] altÄ±nda opsiyonel:
        driver_path = "/opt/homebrew/lib/libmsodbcsql.18.dylib"
      varsa Ã¶nce onu kullanÄ±r.
    - Yoksa 'driver' (Ã¶rn: "ODBC Driver 18 for SQL Server") adÄ±nÄ± kullanÄ±r.
    - BaÄŸlantÄ± kurulamazsa ve 18 deneniyorsa otomatik 17'ye dÃ¼ÅŸer.
    """
    # --- Secrets doÄŸrulama (kullanÄ±cÄ±ya net hata mesajÄ±) ---
    missing_msg = None
    try:
        s = st.secrets["db"]  # type: ignore[index]
    except Exception:
        missing_msg = "`.streamlit/secrets.toml` iÃ§inde `[db]` bÃ¶lÃ¼mÃ¼ bulunamadÄ±."
        s = {}
    required = ["server", "username", "password", "database"]
    missing = [k for k in required if not str(s.get(k, "")).strip()]
    if missing_msg or missing:
        try:
            st.error(
                "ğŸ” VeritabanÄ± ayarlarÄ± eksik: " +
                (missing_msg or "") +
                (" Eksik alanlar: " + ", ".join(missing) if missing else "")
            )
        except Exception:
            pass
        raise RuntimeError("Eksik veritabanÄ± secrets ayarlarÄ±")

    server = s["server"]
    user = s["username"]
    pwd = s["password"]
    db = s["database"]
    driver = s.get("driver", "ODBC Driver 18 for SQL Server")
    driver_path = s.get("driver_path")  # opsiyonel, tam yol
    encrypt = s.get("encrypt", False)
    trust_cert = s.get("trust_server_certificate", True)

    # DRIVER token'Ä±nÄ± oluÅŸtur (tam yol Ã¶ncelikli)
    if driver_path:
        driver_token=f"DRIVER={{{driver_path}}};"
    else:
        driver_token=f"DRIVER={{{driver}}};"

    base_conn=(
        f"{driver_token}"
        f"SERVER={server};"
        f"DATABASE={db};"
        f"UID={user};PWD={pwd};"
        f"Encrypt={'yes' if bool(encrypt) else 'no'};"
        f"TrustServerCertificate={'yes' if bool(trust_cert) else 'no'};"
    )

    # Ã–nce birincil ayarla dene
    params=quote_plus(base_conn)
    try:
        engine = create_engine(
            f"mssql+pyodbc:///?odbc_connect={params}",
            pool_pre_ping=True, pool_recycle=1800, pool_size=5, max_overflow=10
        )
        # BaÄŸlantÄ± saÄŸlÄ±ÄŸÄ±: basit SELECT 1
        try:
            from sqlalchemy import text as _sa_text  # local alias
            with engine.connect() as conn:
                conn.execute(_sa_text("SELECT 1"))
        except Exception as ex:
            try:
                st.error(f"ğŸ”Œ VeritabanÄ±na baÄŸlanÄ±lamadÄ± (PRIMARY): {ex}")
            except Exception:
                pass
            raise
        return engine
    except Exception:
        # EÄŸer tam yol kullanÄ±lmadÄ±ysa ve 18 deneniyorsa 17'ye fallback dene
        if (not driver_path) and ("18" in driver):
            fallback_driver="ODBC Driver 17 for SQL Server"
            fallback_token=f"DRIVER={{{fallback_driver}}};"
            conn_fallback=base_conn.replace(driver_token, fallback_token)
            params_fb=quote_plus(conn_fallback)
            engine = create_engine(
                f"mssql+pyodbc:///?odbc_connect={params_fb}",
                pool_pre_ping=True, pool_recycle=1800, pool_size=5, max_overflow=10
            )
            try:
                from sqlalchemy import text as _sa_text  # local alias
                with engine.connect() as conn:
                    conn.execute(_sa_text("SELECT 1"))
            except Exception as ex:
                try:
                    st.error(f"ğŸ”Œ VeritabanÄ±na baÄŸlanÄ±lamadÄ± (FALLBACK): {ex}")
                except Exception:
                    pass
                raise
            return engine
        raise

@st.cache_data(show_spinner=True)
def sql_to_df(sql: str, tag: str = "", params: dict | None = None) -> pd.DataFrame:
    """Run SQL and return DataFrame. tag is an extra cache key to bust cache
    when parameters (e.g., year_min/year_max) change.
    """
    engine=build_engine()
    return pd.read_sql(text(sql), engine, params=params)

# Helper to ensure year range changes invalidate cached DB calls used by UI
# (so Alan daÄŸÄ±lÄ±mÄ± / Yazar tablarÄ± gerÃ§ekten yÄ±l aralÄ±ÄŸÄ±na gÃ¶re yenilenir)


# Build a cache-busting tag that includes year range and current org selections
def _cache_tag_from_state() -> str:
    try:
        y1, y2 = int(year_min), int(year_max)
    except Exception:
        y1, y2 = 0, 0
    def _norm(v):
        if v is None:
            return "-"
        s = str(v)
        # Avoid exploding cache key with long strings
        s = s.strip()
        s = s.replace("|", "/")[:80]
        return s
    uni = _norm(st.session_state.get("y_uni"))
    fac = _norm(st.session_state.get("y_fac"))
    dep = _norm(st.session_state.get("y_dep"))
    sub = _norm(st.session_state.get("y_sub"))
    return f"{y1}-{y2}|{uni}|{fac}|{dep}|{sub}"

def run_sql(sql: str, params: dict | None = None) -> pd.DataFrame:
    try:
        tag = _cache_tag_from_state()
    except Exception:
        tag = ""
    return sql_to_df(sql, tag, params=params)

def plot_barh(labels, values, title, xlabel):
    fig, ax=plt.subplots(figsize=(10, 6))
    ax.barh(labels, values)
    ax.set_title(title)
    ax.set_xlabel(xlabel)
    fig.tight_layout()
    return fig

def make_chart(df, x_col, y_col, kind, title, xlabel=None, ylabel=None):
    fig, ax=plt.subplots(figsize=(10, 6))
    if kind == "Yatay Ã§ubuk":
        ax.barh(df[x_col], df[y_col])
    elif kind == "Dikey Ã§ubuk":
        ax.bar(df[x_col], df[y_col])
    elif kind == "Pasta":
        ax.pie(df[y_col], labels=df[x_col], autopct="%1.1f%%")
    elif kind == "Ã‡izgi":
        ax.plot(df[x_col], df[y_col], marker="o")
    elif kind == "Alan":
        ax.plot(df[x_col], df[y_col], marker="o")
        ax.fill_between(df[x_col], df[y_col], alpha=0.2)
    ax.set_title(title)
    if xlabel:
        ax.set_xlabel(xlabel)
    if ylabel:
        ax.set_ylabel(ylabel)
    fig.tight_layout()
    return fig

# Kolon adlarÄ±nÄ± normalize etme
COLMAPS={
    "alan_yillik": {
        "Alan": {"alan", "field", "category", "subject", "value"},
        "Yil": {"yil", "yÄ±l", "year", "sourcepublishyear"},
        "YayinSayisi": {"yayin", "count", "n", "tot", "toplam", "yayinsayisi"},
    },
    "yazar_yayin": {
        "Yazar": {"yazar", "author", "wosstandard"},
        "YayinSayisi": {"yayin", "count", "n", "tot", "yayinsayisi"},
    },
    "yazar_atif": {
        "Yazar": {"yazar", "author", "wosstandard"},
        "ToplamAtif": {"timescited", "citiationcount", "citations", "toplamatif"},
    },
    "alan_dagilim": {
        "Alan": {"alan", "field", "category", "subject", "value"},
        "YayinSayisi": {"yayin", "count", "n", "tot", "yayinsayisi"},
    },
}

def normalize_columns(df: pd.DataFrame, kind: str) -> pd.DataFrame:
    maps=COLMAPS.get(kind, {})
    if not maps or df is None or df.empty:
        return df
    lower={c.lower(): c for c in df.columns}
    rename={}
    for target, candidates in maps.items():
        for cand in candidates:
            if cand in lower:
                rename[lower[cand]]=target
                break
    if rename:
        df=df.rename(columns=rename)
    return df

def simple_lm_forecast(years, y, target_year: int) -> float:
    """Basit doÄŸrusal regresyon (numpy polyfit). years ve y NumPy array veya liste olabilir."""
    years=np.asarray(years, dtype=float)
    y=np.asarray(y, dtype=float)
    coef=np.polyfit(years, y, 1)  # type: ignore[call-overload]
    slope=float(coef[0])
    intercept=float(coef[1])
    y_pred=slope * float(target_year) + intercept
    return float(max(0.0, y_pred))

# --- Alan adlarÄ±nÄ± tutarlÄ±laÅŸtÄ±rma ve gÃ¼ven aralÄ±ÄŸÄ± hesap yardÄ±mcÄ±larÄ± ---

def _canon_area_key(s: str) -> str:
    """Alan etiketlerini daha tutarlÄ± hale getirmek iÃ§in sade bir anahtar Ã¼retir."""
    if not isinstance(s, str):
        return ""
    t=_strip_tr(s).lower()
    # & ve 'and' eÅŸitle
    t=t.replace("&", " and ")
    t=t.replace("/", " ")
    t=" ".join(t.split())
    # Ã§ok temel eÅŸ anlamlÄ± kÃ¼meler
    SYN={
        "science and technology": "science-technology",
        "technology": "science-technology",
        "engineering": "engineering",
        "computer science": "computer-science",
        "computer sciences": "computer-science",
        "life sciences": "life-sciences",
        "life sciences & biomedicine": "life-sciences",
        "life sciences and biomedicine": "life-sciences",
        "physical sciences": "physical-sciences",
    }
    return SYN.get(t, t)

def _most_frequent(iterable):
    from collections import Counter
    c=Counter([x for x in iterable if pd.notna(x)])
    return c.most_common(1)[0][0] if c else None

def _blend_forecast(y_lm: float | None, y_pr: float | None) -> float:
    """Ä°ki model varsa ortalamasÄ±nÄ±; tek model varsa onu dÃ¶ndÃ¼r."""
    vals=[v for v in [y_lm, y_pr] if v is not None and pd.notna(v)]
    if not vals:
        return float("nan")
    return float(np.mean(vals))

def _volatility_ci(y: np.ndarray, point: float) -> tuple[float, float]:
    """
    YÄ±llÄ±k serinin yÃ¼zde deÄŸiÅŸimlerinin standart sapmasÄ±ndan (Ïƒ) basit bir CI Ã¼ret.
    CI ~ point * (1 Â± Ïƒ). Ã‡ok kÃ¼Ã§Ã¼k veride Ïƒ=0 kabul edilir.
    """
    if y is None or len(y) < 2:
        return max(0.0, float(point)), max(0.0, float(point))
    y=np.asarray(y, dtype=float)
    with np.errstate(divide="ignore", invalid="ignore"):
        pct=np.diff(y) / np.where(y[:-1] == 0, np.nan, y[:-1])
    sigma=float(np.nanstd(pct))
    low=max(0.0, float(point) * (1.0 - sigma))
    high=max(0.0, float(point) * (1.0 + sigma))
    return low, high

# ---- Optional Prophet forecaster (isolated to avoid type-checker noise) ----
from typing import Optional

def _prophet_forecast_safe(years: pd.Series | np.ndarray | list, values: pd.Series | np.ndarray | list, target_year: int) -> Optional[float]:
    """Return Prophet forecast for target_year or None if Prophet unavailable or fails.
    This function imports Prophet locally and keeps types loose to avoid Pylance complaints.
    """
    if not PROPHET_OK:
        return None
    try:
        # Local import keeps global type as unknown for static checker
        from prophet import Prophet as _Prophet  # type: ignore
        import pandas as _pd  # local alias to avoid name shadowing
        years_arr = np.asarray(years)
        vals_arr = np.asarray(values, dtype=float)
        if len(years_arr) < 3:
            return None
        dfp = _pd.DataFrame({"ds": _pd.to_datetime(years_arr, format="%Y"), "y": vals_arr})
        m = _Prophet(yearly_seasonality=False, weekly_seasonality=False, daily_seasonality=False)  # type: ignore[arg-type]
        m.fit(dfp)
        fut = _pd.DataFrame({"ds": [_pd.to_datetime(f"{int(target_year)}-01-01")]})
        yhat = float(m.predict(fut)["yhat"].iloc[0])
        return yhat
    except Exception:
        return None

# ----------------- Sidebar -----------------
with st.sidebar:
    st.header("Veri KaynaÄŸÄ±")
    st.caption("Bu uygulama veriyi doÄŸrudan **SQL Server**â€™dan Ã§eker.")
    source="SQL Server"

    st.divider()
    st.header("Parametreler")

    # Prophet'i isteÄŸe baÄŸlÄ± yap
    _use_prophet = st.checkbox("Prophet'i kullan", value=False,
                                help="SeÃ§iliyse Prophet modeli Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r. SeÃ§ilmezse yalnÄ±zca doÄŸrusal model (LM) Ã§alÄ±ÅŸÄ±r.")
    st.session_state["use_prophet"] = bool(_use_prophet)
    if not _use_prophet:
        PROPHET_OK = False  # Prophet import edilmiÅŸ olsa bile hesaplamayÄ± devre dÄ±ÅŸÄ± bÄ±rak

    # Mevcut veri aralÄ±ÄŸÄ±nÄ± otomatik keÅŸfet (her zaman SQL Server)
    cur_year=datetime.date.today().year
    min_bound, max_bound=db_year_bounds()

    # KullanÄ±cÄ± arayÃ¼zÃ¼: sÄ±nÄ±rlar veri aralÄ±ÄŸÄ±na sabitlenir
    st.markdown(
        "<small style='color: #666;'>Verilerin hangi yÄ±ldan baÅŸlayarak gÃ¶sterileceÄŸini seÃ§in. En eski yÄ±l, elimizdeki ilk veri yÄ±lÄ±dÄ±r.</small>",
        unsafe_allow_html=True
    )
    # VarsayÄ±lan baÅŸlangÄ±Ã§: 2019 (eÄŸer aralÄ±k dÄ±ÅŸÄ±nda kalÄ±yorsa sÄ±nÄ±rlar iÃ§inde tutulur)
    default_start=int(min(max(2019, int(min_bound)), int(max_bound)))
    year_min=st.number_input(
        "BaÅŸlangÄ±Ã§ yÄ±lÄ±",
        int(min_bound),
        int(max_bound),
        default_start,
        1
    )
    # En eski yÄ±l bilgisini kullanÄ±cÄ±ya not olarak gÃ¶ster
    st.caption(
        f"ğŸ“Œ Bu veritabanÄ±nda gÃ¶rÃ¼len en eski yÄ±l: **{int(min_bound)}**. Daha geriye gidilemez; istersen {int(min_bound)}'e kadar Ã§ekebilirsin.")
    st.markdown(
        "<small style='color: #666;'>Verilerin hangi yÄ±la kadar gÃ¶sterileceÄŸini seÃ§in. Gelecekteki yÄ±llar seÃ§ilemez.</small>",
        unsafe_allow_html=True
    )
    year_max=st.number_input("BitiÅŸ yÄ±lÄ±", int(
        min_bound), int(max_bound), int(max_bound), 1)

    # --- YÄ±l aralÄ±ÄŸÄ± deÄŸiÅŸtiyse cache'i temizle ve Ã§Ä±ktÄ±larÄ± gÃ¼ncelle ---
    prev_key_min="_prev_year_min"
    prev_key_max="_prev_year_max"
    prev_min=st.session_state.get(prev_key_min)
    prev_max=st.session_state.get(prev_key_max)
    if prev_min is None or prev_max is None:
        st.session_state[prev_key_min]=int(year_min)
        st.session_state[prev_key_max]=int(year_max)
    else:
        if int(year_min) != int(prev_min) or int(year_max) != int(prev_max):
            st.session_state[prev_key_min]=int(year_min)
            st.session_state[prev_key_max]=int(year_max)
            try:
                st.cache_data.clear()
            except Exception:
                pass
            st.session_state["ran"]=True

    # Kurumsal seÃ§imler deÄŸiÅŸince de Ã¶nbelleÄŸi temizleyelim (dropdown anahtarlarÄ±)
    for key in ("y_uni", "y_fac", "y_dep", "y_sub"):
        if key in st.session_state:
            # Her yeniden Ã§izimde bu deÄŸerler deÄŸiÅŸtiyse cache'i sÄ±fÄ±rlamak gÃ¼venli
            st.session_state.setdefault(f"_prev_{key}", st.session_state[key])
            if st.session_state[f"_prev_{key}"] != st.session_state[key]:
                try:
                    st.cache_data.clear()
                except Exception:
                    pass
                st.session_state[f"_prev_{key}"]=st.session_state[key]
                st.session_state["ran"]=True

    cur_year = datetime.date.today().year
    # KullanÄ±cÄ± geÃ§miÅŸ yÄ±llar iÃ§in de tahmin yapabilsin: alt sÄ±nÄ±r veri kÃ¼mesindeki en eski yÄ±l
    min_ty = int(min_bound)
    max_ty = int(cur_year + 10)  # en fazla bugÃ¼nden +10 yÄ±l
    # VarsayÄ±lan: bugÃ¼nÃ¼n yÄ±lÄ±ndan bir sonraki yÄ±l (aralÄ±k dÄ±ÅŸÄ±na taÅŸarsa sÄ±nÄ±rlar iÃ§inde tut)
    default_ty = int(cur_year + 1)
    if default_ty < min_ty:
        default_ty = min_ty
    if default_ty > max_ty:
        default_ty = max_ty
    target_year = st.number_input(
        "Tahmin yÄ±lÄ±",
        min_ty,
        max_ty,
        default_ty,
        1
    )
    st.caption(
        "ğŸ“… GeÃ§miÅŸ veya gelecek bir yÄ±l seÃ§ebilirsiniz. VarsayÄ±lan olarak bugÃ¼nÃ¼n yÄ±lÄ±ndan bir sonraki yÄ±l gelir. Bu bir Ã¶ngÃ¶rÃ¼dÃ¼r; kesin sonuÃ§ deÄŸildir."
    )

    # Model seÃ§imi kaldÄ±rÄ±ldÄ±: her iki model de otomatik Ã§alÄ±ÅŸÄ±r
    st.caption("Tahminler iki modelle hesaplanÄ±r: **DoÄŸrusal (LM)** ve (varsa) **Prophet**. En dÃ¼ÅŸÃ¼k MAE â€˜Ã¶nerilenâ€™ olarak iÅŸaretlenir.")

    top_n=st.slider(
        "Otomatik seÃ§im ve grafikler iÃ§in alan sayÄ±sÄ±", 5, 20, 10, 1)
    run_btn=st.button("ğŸš€ Ã‡alÄ±ÅŸtÄ±r")

    # Run / SÄ±fÄ±rla durumu
    reset_btn=st.button("ğŸ§¹ SÄ±fÄ±rla")
    if reset_btn:
        st.session_state["ran"]=False
    if run_btn:
        st.session_state["ran"]=True
    ran=st.session_state.get("ran", False)

    if not ONE_CLICK:
        st.caption("ğŸ”§ TakÄ±ldÄ±ysa: yÄ±llÄ±k seriyi zorla Ã¼ret")
        if st.button("ğŸ§© YÄ±llÄ±k seriyi yeniden oluÅŸtur", key="force_series_sidebar"):
            try:
                st.cache_data.clear()
            except Exception:
                pass
            st.session_state["ran"]=True
            st.rerun()

    # ONE_CLICK: ilk aÃ§Ä±lÄ±ÅŸta otomatik Ã§alÄ±ÅŸtÄ±r
    if ONE_CLICK and not ran:
        st.session_state["ran"]=True
        ran=True

# --- Konu/Alan attribute adÄ± veritabanÄ±na gÃ¶re deÄŸiÅŸebiliyor. Esnek Ã§Ã¶zÃ¼m:
# VarsayÄ±lan (en yaygÄ±n) konu/alan attribute adlarÄ± â€” fallback
SUBJECT_ATTRS_DEFAULT=[
    "category_info.subject",
    "category_info.subheading",
    "category_info.heading",
]
SUBJECT_CANDIDATES=[
    "category_info.subject",
    "category_info.subheading",
    "category_info.heading",
    "category_info.enhanced-subject",      # bazÄ± ÅŸemalarda sonu 's' olmadan
    "category_info.enhanced-subjects",
]

@st.cache_data(show_spinner=False)
def resolve_subject_attrs(y1: int, y2: int) -> list[str]:
    """
    VeritabanÄ±nda gerÃ§ekten veri dÃ¶ndÃ¼ren konu/alan attribute adlarÄ±nÄ± bul.
    Ã–nce seÃ§ili yÄ±l aralÄ±ÄŸÄ± iÃ§in dener; hiÃ§biri Ã§Ä±kmazsa yÄ±l filtresi olmadan dener;
    yine yoksa varsayÄ±lan listeye dÃ¼ÅŸer.
    """
    working: list[str]=[]

    # 1) SeÃ§ili yÄ±l aralÄ±ÄŸÄ± iÃ§in hÄ±zlÄ± yoklama
    for cand in SUBJECT_CANDIDATES:
        try:
            q = (
                """
                SELECT TOP 1 1 AS ok
                FROM dbo.WosHitAttributes wa
                JOIN dbo.WOSHit wh ON wh.HitId = wa.HitId
                WHERE wa.Name = '""" + _q(cand) + """'
                  AND wh.SourcePublishYear BETWEEN :y1 AND :y2
                """
            )
            df = run_sql(q, params={"y1": int(y1), "y2": int(y2)})
            if not df.empty:
                working.append(cand)
        except Exception:
            pass
    if working:
        return working

    # 2) YÄ±l filtresi olmadan dene (daha eski kayÄ±tlar iÃ§in)
    for cand in SUBJECT_CANDIDATES:
        try:
            q=f"SELECT TOP 1 1 AS ok FROM dbo.WosHitAttributes WHERE Name = '{_q(cand)}'"
            df=run_sql(q)
            if not df.empty:
                working.append(cand)
        except Exception:
            pass
    if working:
        return working

    # 3) Son Ã§are: en yaygÄ±n Ã¶ntanÄ±mlÄ± adlar
    return SUBJECT_ATTRS_DEFAULT

# Basit SQL tÄ±rnak kaÃ§Ä±ÅŸÄ± (tek tÄ±rnaklarÄ± iki tek tÄ±rnak yapar)

def _q(v: str) -> str:
    return v.replace("'", "''") if isinstance(v, str) else v

def _in_list_sql(strs: list[str]) -> str:
    return ", ".join(f"'{_q(s)}'" for s in strs)

# ---- Kurumsal filtreleme iÃ§in yardÄ±mcÄ±lar (varsayÄ±lan attribute adlarÄ±) ----
ORG_CFG=st.secrets.get("org", {}) if hasattr(st, "secrets") else {}
UNI_ATTR=ORG_CFG.get("uni_attr", "affiliation.organization")
FAC_ATTR=ORG_CFG.get("faculty_attr", "affiliation.suborganization")
DEPT_ATTR=ORG_CFG.get("department_attr", "affiliation.suborganization_2")
SUB_ATTR=ORG_CFG.get("subunit_attr", "affiliation.suborganization_3")

_DEF_ALL="â€” TÃ¼mÃ¼ â€”"

# ---- YOKSIS / CUAUTHOR tabanlÄ± kurumsal filtreleme yardÄ±mcÄ±larÄ± ----
ORG2_CFG=st.secrets.get("org2", {}) if hasattr(st, "secrets") else {}
ORG2_SCHEMA=ORG2_CFG.get("schema", "dbo")

def _qual(tbl: str) -> str:
    """Åema adÄ±nÄ± ekle (Ã¶rn. dbo.tablo). Zaten noktalÄ±ysa dokunma."""
    return tbl if "." in tbl else f"{ORG2_SCHEMA}.{tbl}"

YOKSIS_TBL=ORG2_CFG.get("yoksis_table", "yoksisbirim")
Y_ID=ORG2_CFG.get("id_col", "BirimID")
Y_PARENT=ORG2_CFG.get("parent_col", "UstBirimID")
Y_NAME=ORG2_CFG.get("name_col", "BirimAdi")
Y_LEVEL=ORG2_CFG.get("level_col", "Duzey")

Y_L_UNI=int(ORG2_CFG.get("level_uni", 1))
Y_L_FAC=int(ORG2_CFG.get("level_fac", 2))
Y_L_DEP=int(ORG2_CFG.get("level_dept", 3))
Y_L_SUB=int(ORG2_CFG.get("level_sub", 4))

# VarsayÄ±lan birleÅŸik dÃ¼zey kÃ¼meleri (yaygÄ±n kullanÄ±m)
UNI_LEVELS_DEF=[Y_L_UNI]
FAC_LEVELS_DEF=list(dict.fromkeys([Y_L_FAC, 9]))
DEPT_LEVELS_DEF=list(dict.fromkeys([Y_L_DEP, 10, 13]))
SUB_LEVELS_DEF=list(dict.fromkeys([Y_L_SUB, 10, 13]))

# secrets.toml iÃ§inde [org2] altÄ±nda fac_levels / dept_levels / sub_levels / uni_levels listeleri tanÄ±mlanabilir

def _levels_from_secret(key: str, default: list[int]) -> list[int]:
    vals=ORG2_CFG.get(key, default)
    try:
        out=[int(x) for x in vals] if isinstance(
            vals, (list, tuple)) else default
    except Exception:
        out=default
    # benzersiz ve sÄ±ralÄ± tut
    return list(dict.fromkeys(out))

UNI_LEVELS=_levels_from_secret("uni_levels", UNI_LEVELS_DEF)
FAC_LEVELS=_levels_from_secret("fac_levels", FAC_LEVELS_DEF)
DEPT_LEVELS=_levels_from_secret("dept_levels", DEPT_LEVELS_DEF)
SUB_LEVELS=_levels_from_secret("sub_levels", SUB_LEVELS_DEF)


CUAUTHOR_TBL=ORG2_CFG.get("cuauthor_table", "cuauthor")
CA_HIT=ORG2_CFG.get("hitid_col")  # opsiyonel
CA_UNIT=ORG2_CFG.get("unit_id_col", "BirimID")
CA_AUTHOR=ORG2_CFG.get("author_id_col")  # opsiyonel

# Otomatik tespit: EÄŸer CA_HIT / CA_AUTHOR secrets'ta yoksa, tablodan bulmayÄ± dene
if not CA_HIT or not CA_AUTHOR:
    _det_hit, _det_auth = detect_cuauthor_mapping()
    if _det_hit and not CA_HIT:
        CA_HIT = _det_hit
    if _det_auth and not CA_AUTHOR:
        CA_AUTHOR = _det_auth

WOSAUTHOR_TBL=ORG2_CFG.get("wos_author_table", "WosAuthor")
WA_AUTHOR=ORG2_CFG.get("wa_author_id_col", "AuthorId")
WA_NAME=ORG2_CFG.get("wa_name_col", "wosStandard")
# Researcher ID kolonu her veritabanÄ±nda bulunmayabilir â†’ opsiyonel yap
WA_RID=ORG2_CFG.get("wa_researcher_col")
WA_RID_EXPR=f"wa.{WA_RID}" if WA_RID else "NULL"

@st.cache_data(show_spinner=False)
def load_yoksis_df() -> pd.DataFrame:
    # CSV fallback
    if _csv_exists("yoksisbirim.csv"):
        df = _csv_load("yoksisbirim.csv").rename(columns={
            "BirimID": "id", "UstBirimID": "parent_id", "BirimAdi": "name", "Duzey": "lvl",
            "Tur": "tur"  # allow mapping from text levels
        })
    else:
        try:
            df = sql_to_df(
                f"SELECT * FROM {_qual(YOKSIS_TBL)}"
            )
        except Exception as ex:
            try:
                st.warning(f"YOKSIS sorgusu baÅŸarÄ±sÄ±z: {_qual(YOKSIS_TBL)} â€” {ex}")
            except Exception:
                pass
            return pd.DataFrame(columns=["id", "parent_id", "name", "lvl"])  # empty

    if df is None or df.empty:
        return pd.DataFrame(columns=["id", "parent_id", "name", "lvl"])  # empty

    # Standardize likely column names from your schema (YoksisId, UstYoksisId, Ad, Tur, AdIng ...)
    rename_map = {
        Y_ID: "id",
        Y_PARENT: "parent_id",
        Y_NAME: "name",
        Y_LEVEL: "lvl",
        "YoksisId": "id",
        "UstYoksisId": "parent_id",
        "BirimID": "id",
        "UstBirimID": "parent_id",
        "Ad": "name",
        "BirimAdi": "name",
        "Tur": "tur",
    }
    for k, v in list(rename_map.items()):
        if k in df.columns:
            df = df.rename(columns={k: v})

    # Coerce numeric types where appropriate
    for c in ("id", "parent_id"):
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    # If a numeric `lvl` is not present, derive it from textual `tur`
    if "lvl" not in df.columns or df["lvl"].isna().all():
        lvl_map = {
            # Turkish variants
            "universite": 1, "uÌˆniversite": 1, "Ã¼niversite": 1,
            "fakulte": 2, "fakuÌˆlte": 2, "fakÃ¼lte": 2,
            "boÌˆluÌˆm": 3, "bÃ¶lÃ¼m": 3, "bolum": 3, "department": 3,
            "anabilim dali": 4, "anabilim dalÄ±": 4, "abd": 4, "division": 4,
            # English
            "university": 1, "faculty": 2, "school": 2, "college": 2,
        }
        def _to_lvl(x: object) -> float:
            if not isinstance(x, str):
                return float("nan")
            key = _strip_tr(x).lower()
            return float(lvl_map.get(key, float("nan")))
        if "tur" in df.columns:
            df["lvl"] = df["tur"].map(_to_lvl)

    # Keep only required columns
    keep = [c for c in ["id", "parent_id", "name", "lvl"] if c in df.columns]
    df = df[keep].dropna(subset=["id"]).copy()
    return df

@st.cache_data(show_spinner=False)
def yoksis_options(level: int, parent_id: int | None=None) -> pd.DataFrame:
    df=load_yoksis_df()
    if df.empty:
        return df
    out=df[df["lvl"] == level]
    if parent_id is not None:
        out=out[out["parent_id"] == int(parent_id)]
    return out.sort_values(by="name", kind="stable")

# Birden fazla dÃ¼zey iÃ§in (birleÅŸtirilmiÅŸ), parent_id opsiyonel
@st.cache_data(show_spinner=False)
def yoksis_options_any(levels: list[int], parent_id: int | None=None) -> pd.DataFrame:
    df=load_yoksis_df()
    if df.empty:
        return df
    out=df[df["lvl"].isin(levels)]
    if parent_id is not None:
        out=out[out["parent_id"] == int(parent_id)]
    return out.sort_values(by="name", kind="stable")

@st.cache_data(show_spinner=False)
def yoksis_descendants(root_ids: list[int]) -> list[int]:
    df=load_yoksis_df()
    if df.empty:
        return []
    ids=set(int(x) for x in root_ids)
    added=True
    while added:
        added=False
        children=df[df["parent_id"].isin(ids)]["id"].astype(int).tolist()
        for c in children:
            if c not in ids:
                ids.add(c)
                added=True
    return sorted(ids)

# WHERE parÃ§asÄ± Ã¼ret (cuauthor ile)

def org_where_cuauthor(selected_ids: list[int]) -> str:
    if not selected_ids:
        return "1=1"  # filtre yok
    ids_csv=",".join(str(int(x)) for x in selected_ids)
    return f"ca.{CA_UNIT} IN ({ids_csv})"

# --- Dinamik kurum alanÄ± Ã§Ã¶zÃ¼mleyici ve keÅŸif ---

@st.cache_data(show_spinner=False)
def list_attribute_names(limit: int=1000) -> pd.DataFrame:
    """WosHitAttributes.Name iÃ§in en sÄ±k gÃ¶rÃ¼len isimleri getirir."""
    # CSV fallback
    if _csv_exists("woshitattributes.csv"):
        df = _csv_load("woshitattributes.csv")
        if "Name" in df.columns:
            out = (
                df["Name"].dropna()
                  .value_counts()  # type: ignore[attr-defined]
                  .head(int(limit))
                  .rename_axis("Name").reset_index(name="Cnt")
            )
            return out
    try:
        sql=f"""
            SELECT TOP {limit} Name, COUNT(*) AS Cnt
            FROM dbo.WosHitAttributes
            GROUP BY Name
            ORDER BY Cnt DESC
        """
        return sql_to_df(sql)
    except Exception:
        return pd.DataFrame(columns=["Name", "Cnt"])  # BoÅŸ dÃ¶nÃ¼ÅŸ

# Mevcut seÃ§im haritasÄ±nÄ± al (Ã¶ncelik: session_state -> secrets -> defaults)

def get_org_names() -> dict:
    m=st.session_state.get("org_map")
    if m and all(k in m for k in ("UNI", "FAC", "DEPT", "SUB")):
        return m
    return {
        "UNI": ORG_CFG.get("uni_attr", UNI_ATTR),
        "FAC": ORG_CFG.get("faculty_attr", FAC_ATTR),
        "DEPT": ORG_CFG.get("department_attr", DEPT_ATTR),
        "SUB": ORG_CFG.get("subunit_attr", SUB_ATTR),
    }

# Session'a yaz

def set_org_names(uni: str, fac: str, dept: str, sub: str) -> None:
    st.session_state["org_map"]={"UNI": uni,
        "FAC": fac, "DEPT": dept, "SUB": sub}

# Attribute adlarÄ± iÃ§in olasÄ± varyasyonlar (Ã¶rn. "affiliation.organization" vs "affiliation.organization-enhanced")
def _attr_aliases(attr: str) -> list[str]:
    alts=[attr]
    if attr.startswith("affiliation.") and not attr.endswith("-enhanced"):
        alts.append(attr + "-enhanced")
    return alts

# ---- Kurumsal isim eÅŸleÅŸmesini saÄŸlamlaÅŸtÄ±rmak iÃ§in yardÄ±mcÄ±lar ----
_TR_MAP=str.maketrans({
    "Ã‡": "C", "Ä": "G", "Ä°": "I", "Ã–": "O", "Å": "S", "Ãœ": "U",
    "Ã§": "c", "ÄŸ": "g", "Ä±": "i", "Ã¶": "o", "ÅŸ": "s", "Ã¼": "u",
})

def _strip_tr(s: str) -> str:
    """TÃ¼rkÃ§e karakterleri sadeleÅŸtir, boÅŸluklarÄ± tek boÅŸluÄŸa indir, kÄ±rp."""
    if not isinstance(s, str):
        return ""
    return " ".join(s.translate(_TR_MAP).split()).strip()

# SÄ±k gÃ¶rÃ¼len fakÃ¼lte/birim eÅŸ anlamlÄ±larÄ± (TRâ†’EN)
_TR_EN_SYNONYMS={
    # Institution / generic
    "FAKULTE": ["FACULTY"],
    "FAKULTESI": ["FACULTY"],
    "ENSTITU": ["INSTITUTE"],
    "ENSTITUSU": ["INSTITUTE"],
    "YUKSEKOKUL": ["SCHOOL", "COLLEGE"],
    "YUKSEKOKULU": ["SCHOOL", "COLLEGE"],
    "UNIVERSITESI": ["UNIVERSITY", "UNIV"],
    "UNIVERSITE": ["UNIVERSITY", "UNIV"],
    "UNIV": ["UNIVERSITY"],
    # Broad academic
    "FEN": ["SCIENCE", "SCIENCES"],
    "EDEBIYAT": ["LETTERS", "LITERATURE"],
    "FEN EDEBIYAT": ["SCIENCE AND LETTERS", "SCIENCE & LETTERS"],
    "MUHENDISLIK": ["ENGINEERING"],
    "EGITIM": ["EDUCATION"],
    "TIP": ["MEDICINE", "MEDICAL"],
    "ECZACILIK": ["PHARMACY"],
    "DIS HEKIMLIGI": ["DENTISTRY", "DENTAL"],
    "ZIRAAT": ["AGRICULTURE", "AGRICULTURAL"],
    "IKTISAT": ["ECONOMICS"],
    "IDARI BILIMLER": ["ADMINISTRATIVE SCIENCES", "ADMINISTRATIVE"],
    "ILETISIM": ["COMMUNICATION"],
    "HUKUK": ["LAW"],
    "MIMARLIK": ["ARCHITECTURE", "ARCH"],
    # Common engineering departments
    "INS AAT": ["CIVIL"],     # tokenisation may split as ["INS", "AAT"]
    "INSAAT": ["CIVIL"],
    "BILGISAYAR": ["COMPUTER"],
    "YAZILIM": ["SOFTWARE"],
    "ELEKTRIK": ["ELECTRICAL", "ELECTRIC"],
    "ELEKTRONIK": ["ELECTRONICS"],
    "ELEKTRIK ELEKTRONIK": ["ELECTRICAL AND ELECTRONICS", "ELECTRICAL & ELECTRONICS"],
    "MAKINE": ["MECHANICAL"],
    "ENDUSTRI": ["INDUSTRIAL"],
    "CEVRE": ["ENVIRONMENTAL"],
    "GIDA": ["FOOD"],
    "JEOMATIK": ["GEOMATICS"],
    "HARITA": ["SURVEYING", "GEODESY"],
    "MATERIALS": ["MATERIALS"],  # allow English token to persist
    # Health/other common departments
    "HEM S IREL IK": ["NURSING"],
    "HEMSIRELIK": ["NURSING"],
    "VETERINER": ["VETERINARY"],
    # Sciences
    "KIMYA": ["CHEMISTRY"],
    "FIZIK": ["PHYSICS"],
    "MATEMATIK": ["MATHEMATICS"],
    "BIYOLOJI": ["BIOLOGY", "LIFE SCIENCES"],
}

def _tokens(s: str) -> list[str]:
    """Kurum adÄ±nÄ± sadeleÅŸtirip majuskÃ¼l tokenlere ayÄ±rÄ±r (harf/rakam)."""
    import re
    s2=_strip_tr(s).upper()
    toks=re.findall(r"[A-Z0-9]+", s2)
    # Drop generic structural words that rarely appear in English address strings
    STOPWORDS={
        "BOLUM", "BOLUMU", "ANABILIM", "ANABILIMDALI", "ANA", "BILIM", "DALI",
        "PROGRAM", "PROGRAMI", "ENSTITU", "ENSTITUSU", "YUKSEKOKUL", "YUKSEKOKULU",
        "FAKULTE", "FAKULTESI"
    }
    toks=[t for t in toks if t not in STOPWORDS]
    return toks

def _wildcard_from_tokens(toks: list[str]) -> str:
    """SÄ±ralÄ± tokenlerden %A%B%C% deseni Ã¼retir."""
    if not toks:
        return ""
    return "%" + "%".join(toks) + "%"

def _synonym_tokens(toks: list[str]) -> list[list[str]]:
    """TRâ†’EN eÅŸ anlamlÄ±larla alternatif token dizileri oluÅŸtur."""
    # Basit strateji: her token iÃ§in eÅŸ anlamlÄ±larÄ± varsa yerlerine de koy
    outs=[toks]
    # birleÅŸtirilmiÅŸ stringlerde TRâ†’EN map denemeleri
    for tr, ens in _TR_EN_SYNONYMS.items():
        tr_toks=tr.split()
        # alt dizi olarak geÃ§iyorsa varyasyon ekle
        for i in range(0, max(1, len(toks) - len(tr_toks) + 1)):
            if toks[i:i+len(tr_toks)] == tr_toks:
                for en in ens:
                    new=toks[:i] + en.split() + toks[i+len(tr_toks):]
                    outs.append(new)
    # Tekil eÅŸ anlamlÄ± yer deÄŸiÅŸimleri
    for i, t in enumerate(toks):
        if t in _TR_EN_SYNONYMS:
            for en in _TR_EN_SYNONYMS[t]:
                new=toks.copy(); new[i:i+1]=en.split()
                outs.append(new)
    # Yinelenenleri kaldÄ±r
    uniq=[]
    seen=set()
    for arr in outs:
        key=" ".join(arr)
        if key not in seen:
            seen.add(key); uniq.append(arr)
    return uniq

def _org_like_patterns(raw: str) -> list[str]:
    """
    Kurum/fakÃ¼lte/birim ismi iÃ§in birden fazla LIKE deseni Ã¼ret.
    Daha kapsayÄ±cÄ± olmasÄ± iÃ§in:
      - TÃ¼rkÃ§e karakterler sadeleÅŸtirilir,
      - TRâ†’EN eÅŸ anlamlÄ±larla varyasyonlar Ã¼retilir,
      - her varyant iÃ§in hem tam-token hem de **3â€“5 harflik prefix** eÅŸleÅŸmeleri eklenir,
      - yaygÄ±n **kÄ±saltmalar** da (ENG, EDU, AGR, PHARM, ARCH, LAW, MED, COMM vb.) denenir.
    """
    toks=_tokens(raw)
    if not toks:
        return []

    variants=_synonym_tokens(toks)
    patterns: list[str]=[]

    # YardÄ±mcÄ±: bir token iÃ§in prefixler Ã¼ret (3â€“5 harf)
    def _prefixes(t: str) -> list[str]:
        t=t.strip()
        out: list[str]=[]
        for k in (5, 4, 3):
            if len(t) >= k:
                out.append(t[:k])
        return out

    # SektÃ¶rel/kurumsal bilinen kÄ±saltmalar (TRâ†’EN kÄ±sa kodlar)
    ABBR={
        "EGITIM": ["EDUC", "EDU", "EGIT", "EGT"],
        "MUHENDISLIK": ["ENG", "MUH"],
        "MIMARLIK": ["ARCH"],
        "ZIRAAT": ["AGR", "AGRI"],
        "FEN": ["SCI", "SC"],
        "EDEBIYAT": ["LIT", "LET"],
        "TIP": ["MED"],
        "ECZACILIK": ["PHARM"],
        "HUKUK": ["LAW"],
        "ILETISIM": ["COMM"],
        "IDARI": ["ADM", "ADMIN"],
        "BILGISAYAR": ["COMP", "CS"],
        "YAZILIM": ["SOFT"],
        "ELEKTRIK": ["ELEC"],
        "ELEKTRONIK": ["ELEC", "ELECTRON"],
        "MAKINE": ["MECH"],
        "ENDUSTRI": ["IND", "IE"],
        "CEVRE": ["ENV", "ENVIR"],
        "KIMYA": ["CHEM"],
        "FIZIK": ["PHYS"],
        "MATEMATIK": ["MATH"],
        "BIYOLOJI": ["BIOL", "LIFE"],
        "VETERINER": ["VET"],
        "DIS": ["DENT", "DENTAL"],
    }

    # 1) SÄ±ralÄ± tÃ¼m tokenler (katÄ± eÅŸleÅŸme): %A%B%C%
    patterns.extend(["%" + "%".join(v) + "%" for v in variants if v])

    # 2) Tek token ve prefix tabanlÄ± esneklik
    all_single_tokens={t for v in variants for t in v}
    for t in all_single_tokens:
        if len(t) >= 3:
            # tam token
            patterns.append(f"%{t}%")
            # prefixler
            for px in _prefixes(t):
                patterns.append(f"%{px}%")
            # bilinen kÄ±saltmalar
            if t in ABBR:
                for ab in ABBR[t]:
                    patterns.append(f"%{ab}%")

    # 3) VaryantlarÄ±n ilk tokenâ€™i iÃ§in de looser tek-token deseni
    for v in variants:
        if v:
            patterns.append(f"%{v[0]}%")
            for px in _prefixes(v[0]):
                patterns.append(f"%{px}%")

    # 4) Ham metnin sadeleÅŸtirilmiÅŸ hali (tam parÃ§a)
    raw_simpl=_strip_tr(raw)
    if raw_simpl:
        patterns.append(f"%{raw_simpl}%")

    # Yinelenenleri kaldÄ±r
    out: list[str]=[]
    seen: set[str]=set()
    for p in patterns:
        if p and p not in seen:
            seen.add(p)
            out.append(p)
    return out[:80]  # Ã§ok fazla LIKE paterni performansÄ± dÃ¼ÅŸÃ¼rmesin diye sÄ±nÄ±rla



def _addr_like_clause(alias: str, raw: str) -> str:
    """WosAddress.full_address Ã¼zerinde TR/EN varyasyonlarÄ±na gÃ¶re LIKE koÅŸullarÄ± Ã¼retir."""
    pats = _org_like_patterns(raw)
    if not pats:
        return "1=1"
    ors = " OR ".join(
        f"{alias}.full_address COLLATE Turkish_CI_AI LIKE N'{_q(p)}'" for p in pats
    )
    return f"({ors})"

# Strict Ã‡ukurova-only address clause
def _cukurova_addr_clause(alias: str) -> str:
    """Strict filter: only addresses that clearly belong to Cukurova University.
    Matches common variants in TR/EN to avoid hospital or unrelated hits.
    """
    return (
        f"("
        f"{alias}.full_address COLLATE Turkish_CI_AI LIKE N'%Cukurova Univ%' OR "
        f"{alias}.full_address COLLATE Turkish_CI_AI LIKE N'%Cukurova University%' OR "
        f"{alias}.full_address COLLATE Turkish_CI_AI LIKE N'%Ã‡ukurova Ãœniversitesi%'"
        f")"
    )

def _like_join(alias: str, hit_table_alias: str, attr: str, val: str) -> str:
    """
    Kurumsal filtre JOIN'i: attribute isimleri iÃ§in aliaslarÄ± kullanÄ±r ve
    deÄŸer eÅŸleÅŸmesini birden fazla LIKE paterniyle (TRâ†’EN varyasyonlar) yapar.
    """
    names_csv=", ".join(f"'{_q(n)}'" for n in _attr_aliases(attr))
    pats=_org_like_patterns(val)
    if not pats:
        # en azÄ±ndan ham arama kalsÄ±n
        pats=[f"%{_strip_tr(val)}%"]
    ors=" OR ".join(
        f"{alias}.Value COLLATE Turkish_CI_AI LIKE N'{_q(p)}'" for p in pats)
    return (
        f"JOIN dbo.WosHitAttributes {alias} ON {alias}.HitId = {hit_table_alias}.HitId "
        f"AND {alias}.Name IN ({names_csv}) "
        f"AND ({ors})"
    )

# ---- Dinamik attribute value keÅŸfi: seÃ§ilen ad iÃ§in gerÃ§ek deÄŸerleri bul ----

@st.cache_data(show_spinner=False)
def discover_attr_values(attr_name: str, raw: str, limit: int=50) -> list[str]:
    """
    WosHitAttributes iÃ§inde, verilen attribute (Ã¶r. affiliation.suborganization)
    ve kullanÄ±cÄ±nÄ±n seÃ§tiÄŸi ad (raw) iÃ§in yÄ±l aralÄ±ÄŸÄ±nda geÃ§en gerÃ§ek Value'larÄ± keÅŸfeder.
    DÃ¶nen liste doÄŸrudan IN (...) iÃ§inde kullanÄ±labilir.
    """
    try:
        if not raw or raw == _DEF_ALL:
            return []
        pats=_org_like_patterns(raw)
        if not pats:
            return []

        names_csv=", ".join(f"'{_q(n)}'" for n in _attr_aliases(attr_name))
        ors=" OR ".join(
            f"a.Value COLLATE Turkish_CI_AI LIKE N'{_q(p)}'" for p in pats)
        sql = (
            f"""
            SELECT DISTINCT TOP {int(limit)} a.Value AS v
            FROM dbo.WosHitAttributes a
            JOIN dbo.WOSHit wh ON wh.HitId = a.HitId
            WHERE a.Name IN ({names_csv})
              AND ({ors})
              AND wh.SourcePublishYear BETWEEN :y1 AND :y2
            ORDER BY v
            """
        )
        df = run_sql(sql, params={"y1": int(year_min), "y2": int(year_max)})
        vals=[str(x) for x in df.get(
            "v", pd.Series(dtype=object)).dropna().tolist()]
        return vals
    except Exception:
        return []
    pats=_org_like_patterns(raw)
    if not pats:
        pats=[f"%{_strip_tr(raw)}%"]
    ors=" OR ".join(
        f"{alias}.full_address COLLATE Turkish_CI_AI LIKE N'{_q(p)}'" for p in pats)
    return f"({ors})"

# Ãœst seÃ§imlere gÃ¶re attribute deÄŸerlerini getir (kademeli)

@st.cache_data(show_spinner=False)
def distinct_values_for(attr_name: str,
                        sel_uni: str | None=None,
                        sel_fac: str | None=None,
                        sel_dept: str | None=None) -> list[str]:
    try:
        # CSV fallback
        if _csv_exists("woshit.csv") and _csv_exists("woshitattributes.csv"):
            wh = _csv_load("woshit.csv")
            wa = _csv_load("woshitattributes.csv")
            # yÄ±l filtresi
            if "SourcePublishYear" in wh.columns:
                y1, y2 = int(year_min), int(year_max)
                wh = wh[(pd.to_numeric(wh["SourcePublishYear"], errors="coerce") >= y1) & (pd.to_numeric(wh["SourcePublishYear"], errors="coerce") <= y2)]
            names = get_org_names()
            # hedef attr belirle
            if attr_name == UNI_ATTR:
                target = names["UNI"]
            elif attr_name == FAC_ATTR:
                target = names["FAC"]
            elif attr_name == DEPT_ATTR:
                target = names["DEPT"]
            else:
                target = names["SUB"]
            # alias'lÄ± isim varyasyonlarÄ±
            candidates = _attr_aliases(target)
            cur = wa[wa["Name"].astype(str).isin(candidates)]
            # JOIN wh
            if "HitId" in wh.columns and "HitId" in cur.columns:
                cur = cur.merge(wh[["HitId"] + (["SourcePublishYear"] if "SourcePublishYear" in wh.columns else [])], on="HitId", how="inner")
            vals = sorted(cur["Value"].dropna().astype(str).unique().tolist()) if "Value" in cur.columns else []
            return vals
        names=get_org_names()
        # Hedef attr'Ä±, mevcut haritaya gÃ¶re belirle
        if attr_name == UNI_ATTR:
            target=names["UNI"]
        elif attr_name == FAC_ATTR:
            target=names["FAC"]
        elif attr_name == DEPT_ATTR:
            target=names["DEPT"]
        else:
            target=names["SUB"]
        # JOIN parÃ§alarÄ± da gÃ¼ncel isimlere gÃ¶re kurulsun
        joins=[]
        if sel_uni and sel_uni != _DEF_ALL:
            joins.append(_like_join("orgu", "wh", names["UNI"], sel_uni))
        if sel_fac and sel_fac != _DEF_ALL:
            joins.append(_like_join("orgf", "wh", names["FAC"], sel_fac))
        if sel_dept and sel_dept != _DEF_ALL:
            joins.append(_like_join("orgd", "wh", names["DEPT"], sel_dept))
        joins_sql="\n".join(joins)
        names_csv=", ".join(f"'{_q(n)}'" for n in _attr_aliases(target))
        sql = (
            """
            SELECT DISTINCT cur.Value AS Val
            FROM dbo.WOSHit wh
            JOIN dbo.WosHitAttributes cur ON cur.HitId = wh.HitId AND cur.Name IN (""" + names_csv + """)
            """ + joins_sql + """
            WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
            ORDER BY Val
            """
        )
        dfv = run_sql(sql, params={"y1": int(year_min), "y2": int(year_max)})
        vals=[str(x) for x in dfv.get("Val", pd.Series(dtype=object)).dropna().tolist()]
        if vals:
            return vals
        # Fallback: bazÄ± kurulumlarda Name sÃ¼tunu `affiliation.organizationX` gibi varyantlara sahip olabiliyor.
        # Bu durumda Name iÃ§in LIKE prefix eÅŸleÅŸmesi dene (case-insensitive).
        names_like = " OR ".join(
            [f"LOWER(cur.Name) LIKE LOWER('{_q(n)}%')" for n in _attr_aliases(target)]
        )
        sql_like = (
            """
            SELECT DISTINCT cur.Value AS Val
            FROM dbo.WOSHit wh
            JOIN dbo.WosHitAttributes cur ON cur.HitId = wh.HitId
            """ + ("\n" + joins_sql if joins_sql else "") + "\n" +
            """
            WHERE (""" + names_like + ")\n"
            "  AND wh.SourcePublishYear BETWEEN :y1 AND :y2\n"
            "ORDER BY Val\n"
        )
        dfv2 = run_sql(sql_like, params={"y1": int(year_min), "y2": int(year_max)})
        vals2=[str(x) for x in dfv2.get("Val", pd.Series(dtype=object)).dropna().tolist()]
        return vals2
    except Exception:
        return []

# SeÃ§imlere gÃ¶re JOIN parÃ§alarÄ±nÄ± Ã¼ret
def build_org_joins(selected_uni: str | None,
                    selected_fac: str | None,
                    selected_dept: str | None,
                    selected_sub: str | None,
                    addr_alias: str="wa") -> str:
    clauses: list[str]=[]
    # Her durumda: sadece Ã‡ukurova Ãœniversitesi adresleri (hastane gibi kurumlarÄ± dÄ±ÅŸla)
    clauses.append(_cukurova_addr_clause(addr_alias))
    # full_address iÃ§inde hiyerarÅŸik tÃ¼m bilgiler bulunduÄŸu iÃ§in, her seÃ§ili dÃ¼zeyi
    # ayrÄ± bir LIKE grubu olarak AND ile baÄŸlarÄ±z. Her grubun iÃ§inde TRâ†’EN varyasyonlarÄ± OR ile birleÅŸir.
    if selected_uni and selected_uni != _DEF_ALL:
        clauses.append(_addr_like_clause(addr_alias, selected_uni))
    if selected_fac and selected_fac != _DEF_ALL:
        clauses.append(_addr_like_clause(addr_alias, selected_fac))
    if selected_dept and selected_dept != _DEF_ALL:
        clauses.append(_addr_like_clause(addr_alias, selected_dept))
    if selected_sub and selected_sub != _DEF_ALL:
        clauses.append(_addr_like_clause(addr_alias, selected_sub))

    if not clauses:
        return ""  # kurumsal filtre yok

    where_part=" AND ".join(clauses)
    # Tek bir JOIN ile tÃ¼m adres filtrelerini uygula
    return f"\nJOIN dbo.WosAddress {addr_alias} ON {addr_alias}.HitId = wh.HitId AND {where_part}"

# Yeni: WosHitAttributes tabanlÄ± kurumsal JOIN'ler (attribute-based)

def build_org_joins_attrs(selected_uni, selected_fac, selected_dept, selected_sub) -> str:
    """
    WosHitAttributes tabanlÄ± kurumsal JOIN'ler.
    - Ã–nce seÃ§ime karÅŸÄ±lÄ±k gelen gerÃ§ek Value'larÄ± dinamik olarak keÅŸfeder (discover_attr_values).
    - DeÄŸer listesi bulunamazsa LIKE varyasyonlarÄ±yla eÅŸleÅŸir.
    """
    names=get_org_names()
    joins: list[str]=[]

    # Ãœniversite
    if selected_uni and selected_uni != _DEF_ALL:
        vals=discover_attr_values(names["UNI"], selected_uni)
        if vals:
            joins.append(
                f"JOIN dbo.WosHitAttributes orgu ON orgu.HitId = wh.HitId "
                f"AND orgu.Name IN ({_in_list_sql(_attr_aliases(names['UNI']))}) "
                f"AND orgu.Value IN ({_in_list_sql(vals)})"
            )
        else:
            joins.append(_like_join("orgu", "wh", names["UNI"], selected_uni))

    # FakÃ¼lte / birim
    if selected_fac and selected_fac != _DEF_ALL:
        vals=discover_attr_values(names["FAC"], selected_fac)
        if vals:
            joins.append(
                f"JOIN dbo.WosHitAttributes orgf ON orgf.HitId = wh.HitId "
                f"AND orgf.Name IN ({_in_list_sql(_attr_aliases(names['FAC']))}) "
                f"AND orgf.Value IN ({_in_list_sql(vals)})"
            )
        else:
            joins.append(_like_join("orgf", "wh", names["FAC"], selected_fac))

    # BÃ¶lÃ¼m / ABD
    if selected_dept and selected_dept != _DEF_ALL:
        vals=discover_attr_values(names["DEPT"], selected_dept)
        if vals:
            joins.append(
                f"JOIN dbo.WosHitAttributes orgd ON orgd.HitId = wh.HitId "
                f"AND orgd.Name IN ({_in_list_sql(_attr_aliases(names['DEPT']))}) "
                f"AND orgd.Value IN ({_in_list_sql(vals)})"
            )
        else:
            joins.append(_like_join(
                "orgd", "wh", names["DEPT"], selected_dept))

    # Alt birim
    if selected_sub and selected_sub != _DEF_ALL:
        vals=discover_attr_values(names["SUB"], selected_sub)
        if vals:
            joins.append(
                f"JOIN dbo.WosHitAttributes orgs ON orgs.HitId = wh.HitId "
                f"AND orgs.Name IN ({_in_list_sql(_attr_aliases(names['SUB']))}) "
                f"AND orgs.Value IN ({_in_list_sql(vals)})"
            )
        else:
            joins.append(_like_join("orgs", "wh", names["SUB"], selected_sub))

    return "\n".join(joins)

# ----------- YENÄ°: Kurumsal filtreli SQL query yardÄ±mcÄ±larÄ± -----------

def build_org_series_sql(
    year_min: int,
    year_max: int,
    selected_uni,
    selected_fac,
    selected_dept,
    selected_sub,
    ids: list[int],
    join_mode: str="addr"
) -> str:
    """
    Alan-yÄ±llÄ±k seri iÃ§in kurumsal filtreli SQL Ã¼retir.
    join_mode: "addr" (adres tabanlÄ±, varsayÄ±lan) veya "attr" (attribute tabanlÄ±).
    """
    if CA_HIT and CA_AUTHOR:
        # CUAUTHOR tabanlÄ± (deÄŸiÅŸmeden)
        return f"""
            SELECT sub.Value AS Alan, wh.SourcePublishYear AS Yil, COUNT(*) AS YayinSayisi
            FROM dbo.WOSHit wh
            JOIN dbo.WosHitAttributes sub ON sub.HitId = wh.HitId AND sub.Name IN ({_in_list_sql(resolve_subject_attrs(int(year_min), int(year_max)))})
            JOIN {_qual(CUAUTHOR_TBL)} ca ON ca.{CA_HIT} = wh.HitId
            WHERE {org_where_cuauthor(ids)}
              AND wh.SourcePublishYear BETWEEN :y1 AND :y2
            GROUP BY sub.Value, wh.SourcePublishYear
            ORDER BY sub.Value, wh.SourcePublishYear;
        """
    else:
        # JOIN seÃ§enekleri
        joins_addr=build_org_joins(
            selected_uni, selected_fac, selected_dept, selected_sub)
        joins_attr=build_org_joins_attrs(
            selected_uni, selected_fac, selected_dept, selected_sub)
        joins_sql=joins_addr if join_mode == "addr" else joins_attr
        return f"""
            SELECT sub.Value AS Alan, wh.SourcePublishYear AS Yil, COUNT(*) AS YayinSayisi
            FROM dbo.WOSHit wh
            JOIN dbo.WosHitAttributes sub ON sub.HitId = wh.HitId AND sub.Name IN ({_in_list_sql(resolve_subject_attrs(int(year_min), int(year_max)))})
            {joins_sql}
            WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
            GROUP BY sub.Value, wh.SourcePublishYear
            ORDER BY sub.Value, wh.SourcePublishYear;
        """


def build_org_researchers_sql(
    year_min: int,
    year_max: int,
    selected_uni,
    selected_fac,
    selected_dept,
    selected_sub,
    ids: list[int],
    join_mode: str="addr"
) -> str:
    """
    Kurumsal filtreli araÅŸtÄ±rmacÄ± listesi SQL.
    join_mode: "addr" (adres tabanlÄ±, varsayÄ±lan) veya "attr" (attribute tabanlÄ±).
    """
    if CA_HIT and CA_AUTHOR:
        # CUAUTHOR tabanlÄ± (deÄŸiÅŸmeden)
        return f"""
            SELECT DISTINCT wa.{WA_NAME} AS Yazar, {WA_RID_EXPR} AS ResearcherID
            FROM {_qual(CUAUTHOR_TBL)} ca
            JOIN {_qual(WOSAUTHOR_TBL)} wa ON wa.{WA_AUTHOR} = ca.{CA_AUTHOR}
            JOIN dbo.WOSHit wh ON wh.HitId = ca.{CA_HIT}
            WHERE {org_where_cuauthor(ids)}
              AND wh.SourcePublishYear BETWEEN :y1 AND :y2
            ORDER BY Yazar;
        """
    else:
        if join_mode == "addr":
            # Use a different alias for address table to avoid clashing with WosAuthor alias 'wa'
            joins_sql=build_org_joins(
                selected_uni, selected_fac, selected_dept, selected_sub, addr_alias="ad")
            return f"""
                SELECT DISTINCT wa.{WA_NAME} AS Yazar, NULL AS ResearcherID
                FROM dbo.WOSHit wh
                JOIN dbo.WosAuthor wa ON wa.HitId = wh.HitId
                {joins_sql}
                WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
                ORDER BY Yazar;
            """
        else:
            # Attribute-based: WosHit JOIN WosAuthor, ardÄ±ndan attribute joinler
            joins_sql=build_org_joins_attrs(
                selected_uni, selected_fac, selected_dept, selected_sub)
            return f"""
                SELECT DISTINCT wa.{WA_NAME} AS Yazar, NULL AS ResearcherID
                FROM dbo.WOSHit wh
                JOIN dbo.WosAuthor wa ON wa.HitId = wh.HitId
                {joins_sql}
                WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
                ORDER BY Yazar;
            """

# -------- YÃ–KSÄ°S-ID tabanlÄ± seri SQL (CuAuthor -> CuAuthorRID -> WosAuthor -> WOSHit) --------

def build_org_series_sql_yoksis(
    year_min: int,
    year_max: int,
    yoksis_ids: list[int],
) -> str:
    """
    SeÃ§ilen YÃ–KSÄ°S dÃ¼ÄŸÃ¼mlerinin (veya torunlarÄ±nÄ±n) ID listesiyle
    alan-yÄ±llÄ±k seri SQL'ini Ã¼retir.
    Zincir: CuAuthor (YoksisId) -> CuAuthorRID (ResearcherID) ->
            WosAuthor (researcherId) -> WOSHit/Attributes.
    """
    if not yoksis_ids:
        # GÃ¼venli tarafta kal: asla boÅŸ IN (...) Ã¼retme
        return ""
    ids_csv = ",".join(str(int(x)) for x in yoksis_ids)
    subjects_csv = _in_list_sql(
        resolve_subject_attrs(int(year_min), int(year_max))
    )
    return f"""
        SELECT sub.Value AS Alan, wh.SourcePublishYear AS Yil, COUNT(*) AS YayinSayisi
        FROM (
            SELECT DISTINCT wa.HitId
            FROM {_qual(CUAUTHOR_TBL)} c
            JOIN {_qual('CuAuthorRID')} cr ON cr.CuAuthorID = c.ID
            JOIN dbo.WosAuthor wa ON wa.researcherId = cr.ResearcherID
            JOIN dbo.WOSHit wh ON wh.HitId = wa.HitId
            WHERE c.YoksisId IN ({ids_csv})
              AND wh.SourcePublishYear BETWEEN :y1 AND :y2
        ) h
        JOIN dbo.WOSHit wh ON wh.HitId = h.HitId
        JOIN dbo.WosHitAttributes sub
          ON sub.HitId = h.HitId AND sub.Name IN ({subjects_csv})
        GROUP BY sub.Value, wh.SourcePublishYear
        ORDER BY sub.Value, wh.SourcePublishYear;
    """

# --------- AraÅŸtÄ±rmacÄ± tablosu gÃ¶sterici yardÄ±mcÄ± ---------

def _render_researchers_table(df: pd.DataFrame) -> None:
    """
    KullanÄ±cÄ± dostu araÅŸtÄ±rmacÄ± tablosu:
    - 'ResearcherID' sÃ¼tununu 'AraÅŸtÄ±rmacÄ± ID' olarak yeniden adlandÄ±rÄ±r.
    - EÄŸer sÃ¼tun tamamen boÅŸ/None ise sÃ¼tunu gizler ve kÃ¼Ã§Ã¼k bir not gÃ¶sterir.
    """
    if df is None or df.empty:
        st.info("SeÃ§ilen filtre iÃ§in araÅŸtÄ±rmacÄ± bulunamadÄ±.")
        return
    df2=df.copy()
    if "ResearcherID" in df2.columns:
        # Normalize: None / "None" / boÅŸ string â†’ NaN
        df2["ResearcherID"]=df2["ResearcherID"].replace(
            {None: pd.NA, "None": pd.NA, "": pd.NA})
        if df2["ResearcherID"].isna().all():
            df2=df2.drop(columns=["ResearcherID"])
            st.caption(
                "â„¹ï¸ Bu veri kÃ¼mesinde **AraÅŸtÄ±rmacÄ± ID** bilgisi bulunamadÄ±; bu sÃ¼tun gizlendi.")
        else:
            df2=df2.rename(columns={"ResearcherID": "AraÅŸtÄ±rmacÄ± ID"})
    st.dataframe(df2, use_container_width=True)

# -------- Kurumsal eÅŸleÅŸme doÄŸrulama yardÄ±mcÄ±larÄ± (addr vs attr) --------

def validation_subjects_and_samples(year_min: int, year_max: int,
                                    selected_uni, selected_fac, selected_dept, selected_sub,
                                    join_mode: str="addr",
                                    top_k: int=25, sample_n: int=40) -> tuple[pd.DataFrame, pd.DataFrame]:
    """
    SeÃ§ilen kurumsal filtreyi kullanarak:
      1) En Ã§ok gÃ¶rÃ¼len alan/konu listesini (top_k) ve
      2) Ã–rnek satÄ±rlarÄ± (sample_n; yÄ±l, alan, full_address) dÃ¶ndÃ¼rÃ¼r.
    join_mode: "addr" veya "attr" (build_org_joins/build_org_joins_attrs kullanÄ±r).
    """
    subjects_csv=_in_list_sql(
        resolve_subject_attrs(int(year_min), int(year_max)))
    if join_mode == "attr":
        joins=build_org_joins_attrs(
            selected_uni, selected_fac, selected_dept, selected_sub)
    else:
        # Adres modunda: adres JOIN'i zaten bu fonksiyon iÃ§inde yapÄ±lÄ±r; alias Ã§akÄ±ÅŸmamasÄ± iÃ§in 'ad' kullan
        joins=build_org_joins(
            selected_uni, selected_fac, selected_dept, selected_sub, addr_alias="ad")

    # 1) Alan daÄŸÄ±lÄ±mÄ± (top_k)
    sql_top = f"""
        SELECT TOP {int(top_k)} sub.Value AS Alan, COUNT(*) AS Kayit
        FROM dbo.WOSHit wh
        JOIN dbo.WosHitAttributes sub
          ON sub.HitId = wh.HitId AND sub.Name IN ({subjects_csv})
        {joins}
        WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
        GROUP BY sub.Value
        ORDER BY Kayit DESC;
    """
    # Ã–rnek satÄ±rlar (yÄ±l/alan/full_address)
    # - addr modunda: adres JOIN'i zaten 'joins' iÃ§inde var ve alias 'ad' olarak geldi â†’ ayrÄ± LEFT JOIN eklemeyiz.
    # - attr modunda: adres JOIN'i yok â†’ burada LEFT JOIN ile 'wa' alias'Ä±nÄ± ekleriz.
    if join_mode == "attr":
        full_addr_select="ISNULL(wa.full_address, '(adres yok)') AS full_address"
        left_join_addr="LEFT JOIN dbo.WosAddress wa ON wa.HitId = wh.HitId"
    else:
        full_addr_select="ISNULL(ad.full_address, '(adres yok)') AS full_address"
        left_join_addr=""  # adres already joined in joins with alias 'ad'

    sql_samples=f"""
        SELECT TOP {int(sample_n)}
            wh.SourcePublishYear AS Yil,
            sub.Value AS Alan,
            {full_addr_select}
        FROM dbo.WOSHit wh
        JOIN dbo.WosHitAttributes sub
          ON sub.HitId = wh.HitId AND sub.Name IN ({subjects_csv})
        {joins}
        {left_join_addr}
        WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
        ORDER BY wh.SourcePublishYear DESC;
    """
    try:
        params = {"y1": int(year_min), "y2": int(year_max)}
        df_top = run_sql(sql_top, params=params)
        df_smp = run_sql(sql_samples, params=params)
    except Exception:
        df_top, df_smp=pd.DataFrame(columns=["Alan", "Kayit"]), pd.DataFrame(
            columns=["Yil", "Alan", "full_address"])

    return normalize_columns(df_top, "alan_dagilim"), normalize_columns(df_smp, "alan_yillik")

# -------- BasitleÅŸtirilmiÅŸ mod bazlÄ± SQL Ã¼retici (tÃ¼m/veri yok durumlarÄ± iÃ§in) --------
def build_org_series_sql_by_mode(
    year_min: int,
    year_max: int,
    selected_uni,
    selected_fac,
    selected_dept,
    selected_sub,
    ids: list[int],
    mode: str = "auto"  # auto | addr | attr | uni_only | none
) -> tuple[str, str]:
    """
    mode:
      - auto: sadece iÅŸaret; SQL dÄ±ÅŸarÄ±da oluÅŸturulur (bu fonksiyon "" dÃ¶ndÃ¼rÃ¼r)
      - addr: WosAddress.full_address ile tÃ¼m seÃ§ili dÃ¼zeyler
      - attr: WosHitAttributes ile tÃ¼m seÃ§ili dÃ¼zeyler
      - uni_only: sadece Ã¼niversite dÃ¼zeyi
      - none: kurumsal filtre yok (tÃ¼m veriler)
    DÃ¶nÃ¼ÅŸ: (sql, human_label)
    """
    subjects_csv = _in_list_sql(
        resolve_subject_attrs(int(year_min), int(year_max))
    )

    # 0) Otomatik modda bu fonksiyon SQL Ã¼retmez
    if mode == "auto":
        return "", "auto"

    # 1) Kurumsal filtre yok â†’ tÃ¼m veri
    if mode == "none":
        sql = f"""
            WITH Y AS (
                SELECT :y1 AS Yil
                UNION ALL
                SELECT Yil + 1 FROM Y WHERE Yil < :y2
            ),
            D AS (
                SELECT sub.Value AS Alan, wh.SourcePublishYear AS Yil, COUNT(*) AS YayinSayisi
                FROM dbo.WOSHit wh
                JOIN dbo.WosHitAttributes sub
                  ON sub.HitId = wh.HitId AND sub.Name IN ({subjects_csv})
                WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
                GROUP BY sub.Value, wh.SourcePublishYear
            ),
            A AS (SELECT DISTINCT Alan FROM D)
            SELECT a.Alan, y.Yil, COALESCE(d.YayinSayisi, 0) AS YayinSayisi
            FROM A a
            CROSS JOIN Y y
            LEFT JOIN D d ON d.Alan = a.Alan AND d.Yil = y.Yil
            ORDER BY a.Alan, y.Yil
            OPTION (MAXRECURSION 0);
        """
        return sql, "tÃ¼mÃ¼"

    # 2) Sadece Ã¼niversite dÃ¼zeyi (adres tabanlÄ± hÄ±zlÄ± yol)
    if mode == "uni_only":
        where_addr = _addr_like_clause("wa", selected_uni) if selected_uni and selected_uni != _DEF_ALL else "1=1"
        join_addr = "" if where_addr == "1=1" else f"JOIN dbo.WosAddress wa ON wa.HitId = wh.HitId AND {where_addr}"
        sql = f"""
            SELECT sub.Value AS Alan, wh.SourcePublishYear AS Yil, COUNT(*) AS YayinSayisi
            FROM dbo.WOSHit wh
            JOIN dbo.WosHitAttributes sub
              ON sub.HitId = wh.HitId AND sub.Name IN ({subjects_csv})
            {join_addr}
            WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
            GROUP BY sub.Value, wh.SourcePublishYear
            ORDER BY sub.Value, wh.SourcePublishYear;
        """
        return sql, f"sadece Ã¼niversite: {selected_uni or 'â€”'}"

    # 3) TÃ¼m seÃ§ili dÃ¼zeylerle â€” adres veya attribute
    if mode in ("addr", "attr"):
        joins = build_org_joins(selected_uni, selected_fac, selected_dept, selected_sub) if mode == "addr" \
                else build_org_joins_attrs(selected_uni, selected_fac, selected_dept, selected_sub)
        sql = f"""
            SELECT sub.Value AS Alan, wh.SourcePublishYear AS Yil, COUNT(*) AS YayinSayisi
            FROM dbo.WOSHit wh
            JOIN dbo.WosHitAttributes sub
              ON sub.HitId = wh.HitId AND sub.Name IN ({subjects_csv})
            {joins}
            WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
            GROUP BY sub.Value, wh.SourcePublishYear
            ORDER BY sub.Value, wh.SourcePublishYear;
        """
        return sql, ("adres" if mode == "addr" else "attribute")

    # VarsayÄ±lan: bilinmeyen mod
    return "", mode
@st.cache_data(show_spinner=False)
def quick_count_for_mode(year_min: int, year_max: int,
                         selected_uni, selected_fac, selected_dept, selected_sub,
                         ids: list[int], mode: str) -> int:
    """Verinin olup olmadÄ±ÄŸÄ±nÄ± hÄ±zlÄ± kontrol etmek iÃ§in COUNT(*) dÃ¶ndÃ¼rÃ¼r."""
    sql, _ = build_org_series_sql_by_mode(year_min, year_max,
                                          selected_uni, selected_fac, selected_dept, selected_sub,
                                          ids, mode)
    if not sql:
        return 0

    # MAXRECURSION ipucunu Ã§Ä±karÄ±p sarmal sayÄ±m yapalÄ±m
    sql_no_opt = sql.replace("OPTION (MAXRECURSION 0);", "")
    q = f"SELECT COUNT(*) AS Cnt FROM ({sql_no_opt}) t"
    try:
        df = run_sql(q, params={"y1": int(year_min), "y2": int(year_max)})
        return int(df['Cnt'].iloc[0]) if not df.empty else 0
    except Exception:
        return 0
# ------ Esnek deneme/gevÅŸetme: kurumsal filtre boÅŸ dÃ¶nerse aÅŸamalÄ± gevÅŸet ------RSION 0)", "")
def try_fetch_org_series(
    year_min: int,
    year_max: int,
    selected_uni,
    selected_fac,
    selected_dept,
    selected_sub,
    ids: list[int],
) -> tuple[pd.DataFrame, str]:
    """
    Kurumsal filtreli yÄ±llÄ±k seriyi getirmeyi dener.
    EÄŸer kullanÄ±cÄ± fakÃ¼lte/bÃ¶lÃ¼m/alt birim dÃ¼zeylerinden **herhangi birini** seÃ§tiyse,
    **katÄ± (strict)** modda sadece tam eÅŸleÅŸmeyi getirir; hiÃ§bir gevÅŸetme yapmaz.
    SeÃ§im yapÄ±lmadÄ±ysa Ã¶nce attr/addr dener, sonra kademeli gevÅŸetir.
    """
    # 0) EÄŸer YÃ–KSÄ°S ID listesi geldiyse, Ã¶nce doÄŸrudan bu yolla dene
    if ids and isinstance(ids, (list, tuple)) and len(ids) > 0:
        try:
            sql_yx = build_org_series_sql_yoksis(int(year_min), int(year_max), [int(x) for x in ids])
            if sql_yx:
                params = {"y1": int(year_min), "y2": int(year_max)}
                df_yx = run_sql(sql_yx, params=params)
                df_yx = normalize_columns(df_yx, "alan_yillik")
                if df_yx is not None and not df_yx.empty:
                    return df_yx, "yoksis-id"
        except Exception:
            pass

    def _lab(u, f, d, s):
        parts = [p for p in [u, f, d, s] if p and p != _DEF_ALL]
        return " > ".join(parts) if parts else "â€”"

    # "â€” TÃ¼mÃ¼ â€”" â†’ None normalizasyonu
    u0 = None if (selected_uni == _DEF_ALL) else selected_uni
    f0 = None if (selected_fac == _DEF_ALL) else selected_fac
    d0 = None if (selected_dept == _DEF_ALL) else selected_dept
    s0 = None if (selected_sub == _DEF_ALL) else selected_sub

    # EÄŸer kullanÄ±cÄ± fakÃ¼lte/bÃ¶lÃ¼m/alt birimden herhangi birini seÃ§tiyse
    # katÄ± mod: sadece tam eÅŸleÅŸmeyi deneriz (Ã¶nce attr sonra addr)
    strict = bool(f0 or d0 or s0)

    attempts: list[tuple[str, str, str | None, str | None, str | None, str | None]] = []
    if strict:
        attempts.append(("attr", "tam", u0, f0, d0, s0))
        attempts.append(("addr", "tam", u0, f0, d0, s0))
    else:
        # 1) Tam filtre (Ã¶nce attr, sonra addr)
        attempts.append(("attr", "tam", u0, f0, d0, s0))
        attempts.append(("addr", "tam", u0, f0, d0, s0))
        # 2) Alt birimi at
        attempts.append(("attr", "alt_yok", u0, f0, d0, None))
        attempts.append(("addr", "alt_yok", u0, f0, d0, None))
        # 3) BÃ¶lÃ¼mÃ¼ de at (sadece fakÃ¼lte + opsiyonel Ã¼niversite)
        attempts.append(("attr", "bolum_yok", u0, f0, None, None))
        attempts.append(("addr", "bolum_yok", u0, f0, None, None))
        # 4) Sadece Ã¼niversite
        attempts.append(("attr", "sadece_uni", u0, None, None, None))
        attempts.append(("addr", "sadece_uni", u0, None, None, None))

    for mode, tag, u, f, d, s in attempts:
        try:
            sql_try = build_org_series_sql(
                int(year_min), int(year_max),
                u, f, d, s,
                ids,
                join_mode=mode
            )
            params = {"y1": int(year_min), "y2": int(year_max)}
            df_try = run_sql(sql_try, params=params)
            df_try = normalize_columns(df_try, "alan_yillik")
            if df_try is not None and not df_try.empty:
                return df_try, f"{mode}/{tag} ({_lab(u, f, d, s)})"
        except Exception:
            continue

    return pd.DataFrame(columns=["Alan", "Yil", "YayinSayisi"]), ("strict" if strict else "none")

def get_df_from_source(kind: str) -> pd.DataFrame:
    # Her zaman SQL Server'dan Ã§ek
    top_n_ext = int(top_n) * 5 if 'top_n' in globals() else 1000
    params = None
    try:
        if kind == "alan_dagilim":
            subjects_csv = _in_list_sql(
                resolve_subject_attrs(int(year_min), int(year_max))
            )
            sql_text = (
                """
                SELECT
                    t.Value AS Alan,
                    COUNT(*) AS YayinSayisi
                FROM (
                    SELECT DISTINCT wa.HitId, wa.Value
                    FROM dbo.WosHitAttributes wa
                    JOIN dbo.WOSHit wh ON wa.HitId = wh.HitId
                    WHERE wa.Name IN (""" + subjects_csv + """)
                      AND wh.SourcePublishYear BETWEEN :y1 AND :y2
                ) AS t
                GROUP BY t.Value
                ORDER BY YayinSayisi DESC;
                """
            )
            params = {"y1": int(year_min), "y2": int(year_max)}
        elif kind == "yazar_yayin":
            sql_text = f"""
                SELECT TOP {top_n_ext} wa.wosStandard AS Yazar, COUNT(*) AS YayinSayisi
                FROM dbo.WosAuthor wa
                JOIN dbo.WOSHit wh ON wa.HitId = wh.HitId
                WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
                GROUP BY wa.wosStandard
                ORDER BY YayinSayisi DESC;
            """
            params = {"y1": int(year_min), "y2": int(year_max)}
        elif kind == "yazar_atif":
            params = {"y1": int(year_min), "y2": int(year_max)}
            cit_col = detect_citation_column()
            if cit_col:
                sql_text = f"""
                    SELECT TOP {top_n_ext}
                        wa.wosStandard AS Yazar,
                        SUM(COALESCE(TRY_CONVERT(int, wh.{cit_col}), 0)) AS ToplamAtif
                    FROM dbo.WosAuthor wa
                    JOIN dbo.WOSHit wh ON wa.HitId = wh.HitId
                    WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
                    GROUP BY wa.wosStandard
                    ORDER BY ToplamAtif DESC;
                """
            else:
                sql_text = f"""
                    SELECT TOP {top_n_ext}
                        wa.wosStandard AS Yazar,
                        CAST(0 AS int) AS ToplamAtif
                    FROM dbo.WosAuthor wa
                    JOIN dbo.WOSHit wh ON wa.HitId = wh.HitId
                    WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
                    GROUP BY wa.wosStandard
                    ORDER BY COUNT(*) DESC;
                """
        elif kind == "alan_yillik":
            subjects_csv = _in_list_sql(
                resolve_subject_attrs(int(year_min), int(year_max))
            )
            sql_text = (
                """
                WITH Y AS (
                    SELECT :y1 AS Yil
                    UNION ALL
                    SELECT Yil + 1 FROM Y WHERE Yil < :y2
                ),
                D AS (
                    SELECT sub.Value AS Alan, wh.SourcePublishYear AS Yil, COUNT(*) AS YayinSayisi
                    FROM dbo.WOSHit wh
                    JOIN dbo.WosHitAttributes sub ON sub.HitId = wh.HitId AND sub.Name IN (""" + subjects_csv + """)
                    WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
                    GROUP BY sub.Value, wh.SourcePublishYear
                ),
                A AS (
                    SELECT DISTINCT Alan FROM D
                )
                SELECT a.Alan, y.Yil, COALESCE(d.YayinSayisi, 0) AS YayinSayisi
                FROM A a
                CROSS JOIN Y y
                LEFT JOIN D d ON d.Alan = a.Alan AND d.Yil = y.Yil
                ORDER BY a.Alan, y.Yil
                OPTION (MAXRECURSION 0);
                """
            )
            params = {"y1": int(year_min), "y2": int(year_max)}
        else:
            sql_text = ""
    except Exception:
        sql_text = ""
    df = run_sql(sql_text, params=params) if sql_text else pd.DataFrame()
    return normalize_columns(df, kind)

# ---- Address-based helpers (WosAddress.full_address) ----
import re

ADDR_SPLIT_RE = re.compile(r"\s*,\s*")

def _addr_parts(addr: str) -> tuple[str | None, str | None, str | None, str | None]:
    """Parse a 'full_address' like 'Cukurova Univ, Faculty of Medicine, Dept X, ...'
    Returns (uni, fac, dept, sub) best-effort.
    """
    if not isinstance(addr, str) or not addr:
        return (None, None, None, None)
    parts = ADDR_SPLIT_RE.split(addr)
    parts = [p.strip() for p in parts if p and p.strip()]
    if not parts:
        return (None, None, None, None)
    uni = parts[0]
    fac = None
    dep = None
    sub = None
    # Heuristics
    for p in parts[1:]:
        pl = p.lower()
        if fac is None and ("fac" in pl or "faculty" in pl or "fak" in pl):
            fac = p
            continue
        if dep is None and ("dept" in pl or "department" in pl or "bÃ¶l" in pl or "bol" in pl or "abd" in pl):
            dep = p
            continue
        if sub is None:
            sub = p
    return (uni, fac, dep, sub)

@st.cache_data(show_spinner=False)
def addr_distinct(level: str,
                  y1: int, y2: int,
                  sel_uni: str | None = None,
                  sel_fac: str | None = None,
                  sel_dept: str | None = None,
                  limit_uni_raw: str | None = None) -> list[str]:
    """Return distinct address components by parsing WosAddress.full_address for hits in year range.
    level: 'uni' | 'fac' | 'dept' | 'sub'
    Parents may be provided to narrow results.
    limit_uni_raw: EÄŸer set edilirse, tÃ¼m dÃ¼zeyleri bu Ã¼niversite ile sÄ±nÄ±rla.
    """
    sql = (
        """
        SELECT DISTINCT wa.full_address AS A
        FROM dbo.WosAddress wa
        JOIN dbo.WOSHit wh ON wh.HitId = wa.HitId
        WHERE wh.SourcePublishYear BETWEEN :y1 AND :y2
        """
    )
    # EÄŸer belirli bir Ã¼niversiteye odaklanmak istiyorsak (Ã¶rn. Ã‡ukurova), adres WHERE koÅŸuluna ekleyelim
    if limit_uni_raw and isinstance(limit_uni_raw, str) and limit_uni_raw.strip():
        # Eski: sql = sql + "\n  AND " + _addr_like_clause("wa", limit_uni_raw.strip())
        # Yeni: Sadece belirli varyasyonlar iÃ§in LIKE ile filtrele
        sql += f"""
          AND (
              wa.full_address COLLATE Turkish_CI_AI LIKE N'%Cukurova Univ%'
              OR wa.full_address COLLATE Turkish_CI_AI LIKE N'%Cukurova University%'
              OR wa.full_address COLLATE Turkish_CI_AI LIKE N'%Ã‡ukurova Ãœniversitesi%'
          )
        """
    df = run_sql(sql, params={"y1": int(y1), "y2": int(y2)})
    if df is None or df.empty or "A" not in df.columns:
        return []
    rows = df["A"].dropna().astype(str).tolist()
    out: list[str] = []
    for a in rows:
        u, f, d, s = _addr_parts(a)
        # EÄŸer belirli bir Ã¼niversiteye odaklanÄ±ldÄ±ysa, yalnÄ±zca ilk parÃ§asÄ± (Ã¼niversite) bu filtreyle uyuÅŸan adresleri kabul et
        in_scope = True
        if limit_uni_raw and isinstance(limit_uni_raw, str) and limit_uni_raw.strip():
            lim = _strip_tr(limit_uni_raw).lower()
            uu = _strip_tr(u or "").lower()
            in_scope = (lim in uu) if uu else False
        if not in_scope:
            continue
        if level == "uni" and u:
            out.append(u)
        elif level == "fac" and f and (sel_uni is None or (u and u == sel_uni)):
            out.append(f)
        elif level == "dept" and d and (sel_uni is None or (u and u == sel_uni)) and (sel_fac is None or (f and f == sel_fac)):
            out.append(d)
        elif level == "sub" and s and (sel_uni is None or (u and u == sel_uni)) and (sel_fac is None or (f and f == sel_fac)) and (sel_dept is None or (d and d == sel_dept)):
            out.append(s)
    # unique + sorted
    uniq = sorted({x.strip(): None for x in out}.keys())
    return uniq

# ----------------- Sekmeler -----------------

tab1, tab2, tab3, tab4, tab5 = st.tabs([
    "Alan daÄŸÄ±lÄ±mÄ±", "Yazar (yayÄ±n)", "Yazar (atÄ±f)", "Alan yÄ±llÄ±k seri", "Tahmin"
])

if ran:
    try:
        # 1) Alan daÄŸÄ±lÄ±mÄ±
        df_alan = get_df_from_source("alan_dagilim")
        with tab1:
            st.subheader("Alanlara gÃ¶re yayÄ±n sayÄ±sÄ±")
            if df_alan is not None and not df_alan.empty and {"Alan", "YayinSayisi"}.issubset(df_alan.columns):
                st.dataframe(df_alan.head(200), use_container_width=True)
                df_top = df_alan.head(int(top_n))
                chart_kind1 = st.selectbox(
                    "Grafik tÃ¼rÃ¼", ["Yatay Ã§ubuk", "Dikey Ã§ubuk", "Pasta"], index=0, key="chart_t1")
                fig = make_chart(df_top, "Alan", "YayinSayisi", chart_kind1,
                                 "En Fazla YayÄ±n YapÄ±lan Alanlar", "Alan", "YayÄ±n SayÄ±sÄ±")
                st.pyplot(fig, use_container_width=True)
                buf = io.BytesIO(); fig.savefig(buf, format="png"); buf.seek(0)
                st.download_button("ğŸ“¥ GrafiÄŸi PNG olarak indir", buf,
                                   "alan_dagilimi.png", "image/png", key="alan_png")
                st.download_button("ğŸ“¥ SonuÃ§ (CSV)", df_alan.to_csv(index=False),
                                   "alan_dagilimi.csv", "text/csv", key="alan_csv")
            else:
                st.info("Beklenen kolonlar bulunamadÄ± veya veri boÅŸ.")

        # 2) En Ã§ok yayÄ±n yapan yazarlar
        df_yazar_yayin = get_df_from_source("yazar_yayin")
        with tab2:
            st.subheader("En Ã§ok yayÄ±n yapan yazarlar")
            if df_yazar_yayin is not None and not df_yazar_yayin.empty and {"Yazar", "YayinSayisi"}.issubset(df_yazar_yayin.columns):
                st.dataframe(df_yazar_yayin.head(200), use_container_width=True)
                df_top = df_yazar_yayin.head(int(top_n))
                chart_kind2 = st.selectbox(
                    "Grafik tÃ¼rÃ¼", ["Yatay Ã§ubuk", "Dikey Ã§ubuk", "Pasta"], index=0, key="chart_t2")
                fig2 = make_chart(df_top, "Yazar", "YayinSayisi", chart_kind2,
                                   "En Ã‡ok YayÄ±n Yapan Yazarlar", "Yazar", "YayÄ±n SayÄ±sÄ±")
                st.pyplot(fig2, use_container_width=True)
                buf = io.BytesIO(); fig2.savefig(buf, format="png"); buf.seek(0)
                st.download_button("ğŸ“¥ GrafiÄŸi PNG olarak indir", buf,
                                   "yazar_yayin.png", "image/png", key="yazar_yayin_png")
                st.download_button("ğŸ“¥ SonuÃ§ (CSV)", df_yazar_yayin.to_csv(index=False),
                                   "yazar_yayin.csv", "text/csv", key="yazar_yayin_csv")
            else:
                st.info("Beklenen kolonlar bulunamadÄ± veya veri boÅŸ.")

        # 3) En Ã§ok atÄ±f alan yazarlar
        df_yazar_atif = get_df_from_source("yazar_atif")
        with tab3:
            st.subheader("En Ã§ok atÄ±f alan yazarlar")
            _cit = detect_citation_column()
            if _cit is None:
                st.caption("â„¹ï¸ Bu veritabanÄ±nda **atÄ±f (TimesCited)** kolonu bulunamadÄ±; bu bÃ¶lÃ¼m yayÄ±n sayÄ±sÄ±na gÃ¶re 0 atÄ±fla gÃ¶sterilir.")
            if df_yazar_atif is not None and not df_yazar_atif.empty and {"Yazar", "ToplamAtif"}.issubset(df_yazar_atif.columns):
                st.dataframe(df_yazar_atif.head(200), use_container_width=True)
                df_top = df_yazar_atif.head(int(top_n))
                chart_kind3 = st.selectbox(
                    "Grafik tÃ¼rÃ¼", ["Yatay Ã§ubuk", "Dikey Ã§ubuk", "Pasta"], index=0, key="chart_t3")
                fig3 = make_chart(df_top, "Yazar", "ToplamAtif", chart_kind3,
                                   "En Ã‡ok AtÄ±f Alan Yazarlar", "Yazar", "Toplam AtÄ±f")
                st.pyplot(fig3, use_container_width=True)
                buf = io.BytesIO(); fig3.savefig(buf, format="png"); buf.seek(0)
                st.download_button("ğŸ“¥ GrafiÄŸi PNG olarak indir", buf,
                                   "yazar_atif.png", "image/png", key="yazar_atif_png")
                st.download_button("ğŸ“¥ SonuÃ§ (CSV)", df_yazar_atif.to_csv(index=False),
                                   "yazar_atif.csv", "text/csv", key="yazar_atif_csv")
            else:
                st.info("Beklenen kolonlar bulunamadÄ± veya veri boÅŸ.")

        # 4) Alan bazlÄ± yÄ±llÄ±k seri
        df_alan_yillik = get_df_from_source("alan_yillik")
        # BoÅŸsa bir kez Ã¶nbellek temizleyip yeniden dene
        if (df_alan_yillik is None) or df_alan_yillik.empty:
            if not st.session_state.get("_auto_rebuilt_series", False):
                try:
                    st.cache_data.clear()
                except Exception:
                    pass
                st.session_state["_auto_rebuilt_series"] = True
                df_alan_yillik = get_df_from_source("alan_yillik")
                if (df_alan_yillik is None) or df_alan_yillik.empty:
                    st.caption("â„¹ï¸ YÄ±llÄ±k seri verisi henÃ¼z gelmedi; seÃ§ili filtrelerde sonuÃ§ olmayabilir.")

        with tab4:
            st.subheader("Alan bazlÄ± yÄ±llÄ±k yayÄ±n sayÄ±sÄ±")
            if df_alan_yillik is not None and not df_alan_yillik.empty and {"Alan", "Yil", "YayinSayisi"}.issubset(df_alan_yillik.columns):
                st.dataframe(df_alan_yillik.head(200), use_container_width=True)
                alanlar = sorted(df_alan_yillik["Alan"].dropna().unique().tolist())
                secilen_alan = st.selectbox("GrafiÄŸini Ã§iz:", alanlar)
                if secilen_alan:
                    tmp = df_alan_yillik[df_alan_yillik["Alan"] == secilen_alan]
                    df_one = tmp.sort_values(by="Yil")  # type: ignore[arg-type]
                    chart_kind4 = st.selectbox(
                        "Grafik tÃ¼rÃ¼", ["Ã‡izgi", "Dikey Ã§ubuk", "Alan"], index=0, key="chart_t4")
                    fig4 = make_chart(df_one, "Yil", "YayinSayisi", chart_kind4,
                                      f"{secilen_alan} â€” YÄ±llÄ±k YayÄ±n SayÄ±sÄ±", "YÄ±l", "YayÄ±n")
                    st.pyplot(fig4, use_container_width=True)
                    buf = io.BytesIO(); fig4.savefig(buf, format="png"); buf.seek(0)
                    st.download_button("ğŸ“¥ GrafiÄŸi PNG olarak indir", buf,
                                       f"alan_yillik_{secilen_alan}.png", "image/png", key="alan_yillik_png")
            else:
                st.info("Beklenen kolonlar bulunamadÄ± veya veri boÅŸ.")

        # 5) Tahmin (yeni: sadece alan seÃ§imi, kurumsal filtre yok)
        with tab5:
            # --- Yeni Tahmin: BÃ¶lÃ¼m/FakÃ¼lte/Birim seÃ§imleri yok ---
            st.subheader(f"ğŸ“ˆ Alan bazlÄ± tahmin â€” hedef yÄ±l: {int(target_year)}")
            st.caption("KullanÄ±cÄ± tÃ¼m alanlar listesinden bir alan seÃ§er; sadece o alan iÃ§in tahmin yapÄ±lÄ±r.")

            # 1) TÃ¼m alanlarÄ±n listesi (df_alan_yillik'ten)
            if df_alan_yillik is None or df_alan_yillik.empty or not {"Alan","Yil","YayinSayisi"}.issubset(df_alan_yillik.columns):
                st.warning("YÄ±llÄ±k seri verisi yok veya beklenen kolonlar bulunamadÄ±. Ã–nce 'Alan yÄ±llÄ±k seri' verisi Ã¼retildiÄŸinden emin olun.")
            else:
                alan_list = (
                    sorted(df_alan_yillik["Alan"].dropna().astype(str).unique().tolist())
                )
                if not alan_list:
                    st.info("Listelenecek alan bulunamadÄ±.")
                else:
                    sec_alan = st.selectbox(
                        "Alan seÃ§in veya yazmaya baÅŸlayÄ±n",
                        options=alan_list,
                        index=0,
                        key="tahmin_alan_select",
                        help="Listenin iÃ§inde yazdÄ±kÃ§a otomatik filtreleme yapÄ±lÄ±r.",
                        placeholder="Ã–rn: phys, comp, bio..."
                    )

                    # 2) SeÃ§ilen alan iÃ§in tek seri
                    ser = (
                        df_alan_yillik[df_alan_yillik["Alan"].astype(str) == str(sec_alan)]
                        .copy()
                        .sort_values(by="Yil")
                    )
                    if ser.empty:
                        st.info("SeÃ§ilen alan iÃ§in seri bulunamadÄ±.")
                    else:
                        # Temel istatistikler ve hedef yÄ±l doÄŸrulamasÄ±
                        years_series = pd.to_numeric(ser["Yil"], errors="coerce").dropna().astype(int)
                        vals_series  = pd.to_numeric(ser["YayinSayisi"], errors="coerce").fillna(0).astype(float)
                        if years_series.empty:
                            st.info("Bu alan iÃ§in yÄ±l bilgisi bulunamadÄ±.")
                            st.stop()

                        # NumPy dizilerine dÃ¶nÃ¼ÅŸtÃ¼r ve aralÄ±k sÄ±nÄ±rlarÄ±nÄ± al
                        years = years_series.to_numpy()
                        vals  = vals_series.to_numpy()
                        min_year = int(years.min())
                        max_year = int(years.max())

                        # EÄŸer hedef yÄ±l eÄŸitim aralÄ±ÄŸÄ±ndaysa (in-sample), gerÃ§ek deÄŸeri al
                        actual_value = None
                        try:
                            mask_in_sample = (years_series == int(target_year))
                            if bool(mask_in_sample.any()):
                                actual_value = float(vals_series[mask_in_sample].iloc[0])
                                st.caption(f"ğŸ“Œ SeÃ§tiÄŸiniz yÄ±l **{int(target_year)}** mevcut veri aralÄ±ÄŸÄ±nda; bu bir **inâ€‘sample** tahmindir (gerÃ§ekle aynÄ± olabilir).")
                        except Exception:
                            actual_value = None

                        if len(years) < 2:
                            st.info("Bu alan iÃ§in en az 2 yÄ±llÄ±k veri gerekli (LM iÃ§in). Daha uzun seri ile daha saÄŸlÄ±klÄ± tahmin yapÄ±lÄ±r.")
                            st.stop()

                        # 3) Tahminler â€” LM zorunlu, Prophet isteÄŸe baÄŸlÄ±
                        y_lm = float("nan")
                        if len(years) >= 2:
                            try:
                                y_lm = simple_lm_forecast(years, vals, int(target_year))
                            except Exception:
                                y_lm = float("nan")

                        y_prop = None
                        if st.session_state.get("use_prophet", False):
                            y_prop = _prophet_forecast_safe(years, vals, int(target_year))

                        y_suggested = _blend_forecast(y_lm, y_prop)
                        base_for_ci = (
                            y_suggested if (y_suggested is not None and not pd.isna(y_suggested)) else
                            (y_lm if not pd.isna(y_lm) else 0.0)
                        )
                        lo, hi = _volatility_ci(vals, float(base_for_ci))

                        # 4) Ã–zet kartlar
                        c1, c2, c3 = st.columns(3)
                        c1.metric(f"LM @ {int(target_year)}", ("â€”" if pd.isna(y_lm) else f"{y_lm:.0f}"))
                        c2.metric(f"Prophet @ {int(target_year)}", ("None" if (y_prop is None or pd.isna(y_prop)) else f"{y_prop:.0f}"))
                        c3.metric("Ã–nerilen", ("â€”" if (y_suggested is None or pd.isna(y_suggested)) else f"{y_suggested:.0f}"))
                        st.caption(f"YaklaÅŸÄ±k gÃ¼ven aralÄ±ÄŸÄ±: {lo:.0f} â€“ {hi:.0f}")
                        # UyarÄ±: veri seyrekliÄŸi / yÃ¼ksek oynaklÄ±k
                        try:
                            total_years = len(years)
                            zero_years = int((vals == 0).sum()) if isinstance(vals, np.ndarray) else 0
                            coverage_ratio = total_years / max(1, (max_year - min_year + 1))  # 0â€“1
                            zero_ratio = zero_years / max(1, total_years)
                            # DeÄŸiÅŸkenlik: bir Ã¶nceki yÄ±la gÃ¶re yÃ¼zde deÄŸiÅŸimlerin std'si
                            with np.errstate(divide="ignore", invalid="ignore"):
                                pct_changes = np.diff(vals) / np.where(vals[:-1] == 0, np.nan, vals[:-1]) if total_years >= 2 else np.array([])
                            sigma = float(np.nanstd(pct_changes)) if pct_changes.size else 0.0

                            sparse_flag = (total_years < 6) or (coverage_ratio < 0.6) or (zero_ratio >= 0.4)
                            volatile_flag = sigma >= 0.6  # oldukÃ§a oynak seri

                            if sparse_flag or volatile_flag:
                                note_parts = []
                                if sparse_flag:
                                    note_parts.append("veri seyrek (uzun boÅŸluklar/Ã§ok az yÄ±l)")
                                if volatile_flag:
                                    note_parts.append("seri Ã§ok oynak")
                                st.warning("âš ï¸ Bu alan iÃ§in " + " ve ".join(note_parts) + "; tahminler **yÃ¼ksek belirsizlik** iÃ§erir.")
                        except Exception:
                            pass

                        # 5) Grafik (tek alan) â€” tahmin noktasÄ± + CI hata Ã§ubuÄŸu ile
                        # Not: make_chart yerine Ã¶zel Ã§izim, Ã§Ã¼nkÃ¼ hedef yÄ±l noktasÄ±na CI ekliyoruz
                        try:
                            # Temel seri grafiÄŸi
                            fig_sel, ax = plt.subplots(figsize=(10, 6))
                            ax.plot(ser["Yil"], ser["YayinSayisi"], marker="o")
                            ax.set_title(f"{sec_alan} â€” YÄ±llÄ±k YayÄ±n SayÄ±sÄ±")
                            ax.set_xlabel("YÄ±l")
                            ax.set_ylabel("YayÄ±n SayÄ±sÄ±")

                            # Y ekseni: negatifleri engelle, tam sayÄ± tick'leri kullan
                            ax.set_ylim(bottom=0)
                            ax.yaxis.set_major_locator(MaxNLocator(integer=True))

                            # Son gÃ¶zlem yÄ±lÄ±nÄ± iÅŸaretle
                            try:
                                last_year = int(ser["Yil"].iloc[-1])
                                last_val = float(ser["YayinSayisi"].iloc[-1])
                                ax.scatter([last_year], [last_val], s=35, color="blue", zorder=3)
                                ax.annotate("Son veri", xy=(last_year, last_val),
                                            xytext=(5, -12), textcoords="offset points")
                            except Exception:
                                pass

                            # LM / Prophet / Ã–nerilen noktalarÄ±nÄ± ayrÄ± iÅŸaretle
                            try:
                                if not pd.isna(y_lm):
                                    ax.scatter([int(target_year)], [float(y_lm)], marker="s", s=45, label="LM", zorder=3)
                                if (y_prop is not None) and (not pd.isna(y_prop)):
                                    ax.scatter([int(target_year)], [float(y_prop)], marker="^", s=55, label="Prophet", zorder=3)
                                if (y_suggested is not None) and (not pd.isna(y_suggested)):
                                    ax.scatter([int(target_year)], [float(y_suggested)], s=65, label="Ã–nerilen", zorder=4)
                                if ax.get_legend_handles_labels()[0]:
                                    ax.legend()
                            except Exception:
                                pass

                            # X eksenini hedef yÄ±lÄ± kapsayacak ÅŸekilde geniÅŸlet
                            try:
                                xmin = int(min(min_year, int(target_year)))
                                xmax = int(max(max_year, int(target_year)))
                                ax.set_xlim(xmin, xmax)
                            except Exception:
                                pass

                            # Tahmin noktasÄ± ve CI (varsa) â€” tek bir hata Ã§ubuÄŸu
                            if (y_suggested is not None) and (not pd.isna(y_suggested)):
                                y0 = float(y_suggested)
                                # errorbar iÃ§in simetrik olmayan yerr kullanÄ±yoruz
                                yerr_low  = max(0.0, y0 - float(lo))
                                yerr_high = max(0.0, float(hi) - y0)
                                ax.errorbar(
                                    [int(target_year)], [y0],
                                    yerr=[[yerr_low], [yerr_high]],
                                    fmt='o', capsize=6, linewidth=1.5
                                )
                                # NoktanÄ±n Ã¼stÃ¼ne kÃ¼Ã§Ã¼k bir etiket
                                ax.annotate(
                                    f"Tahmin: {y0:.0f}",
                                    xy=(int(target_year), y0),
                                    xytext=(5, 8), textcoords='offset points'
                                )

                            fig_sel.tight_layout()
                            st.pyplot(fig_sel, use_container_width=True)
                        except Exception:
                            # Her ihtimale karÅŸÄ± eski basit grafiÄŸe geri dÃ¶n
                            fig_sel = make_chart(ser, "Yil", "YayinSayisi", "Ã‡izgi",
                                                 f"{sec_alan} â€” YÄ±llÄ±k YayÄ±n SayÄ±sÄ±", "YÄ±l", "YayÄ±n SayÄ±sÄ±")
                            st.pyplot(fig_sel, use_container_width=True)

                        # EÄŸer seÃ§ilen yÄ±l geÃ§miÅŸteyse ve gerÃ§ek deÄŸer varsa, hatayÄ± gÃ¶ster
                        if actual_value is not None:
                            err_txt = ""
                            if (y_suggested is not None) and (not pd.isna(y_suggested)):
                                try:
                                    err_val = abs(float(y_suggested) - float(actual_value))
                                    err_txt = f" â€” Tahmin hatasÄ±: Â±{err_val:.0f}"
                                except Exception:
                                    err_txt = ""
                            st.caption(f"ğŸ“Œ GerÃ§ek {int(target_year)}: {actual_value:.0f}{err_txt}")
                        elif actual_value is None and int(target_year) <= max_year:
                            # GeÃ§miÅŸ yÄ±l seÃ§ildi ama seride o yÄ±l yoksa bilgilendir
                            st.caption("ğŸ“Œ SeÃ§tiÄŸiniz yÄ±l veri aralÄ±ÄŸÄ±nda olabilir; bu Ã§Ä±ktÄ± geri-tahmin (hindcast) amaÃ§lÄ±dÄ±r.")

                        # 6) Ä°ndirme: sadece seÃ§ilen alanÄ±n serisi ve tek satÄ±r tahmin
                        col_dl1, col_dl2 = st.columns(2)
                        with col_dl1:
                            st.download_button(
                                "ğŸ“¥ Seri (CSV)", ser.to_csv(index=False), f"seri_{_strip_tr(str(sec_alan))}.csv", "text/csv", key="seri_csv"
                            )
                        with col_dl2:
                            df_pred_row = pd.DataFrame([
                                {
                                    "Alan": sec_alan,
                                    "Yil": int(target_year),
                                    "LM": (None if pd.isna(y_lm) else float(y_lm)),
                                    "Prophet": (None if (y_prop is None or pd.isna(y_prop)) else float(y_prop)),
                                    "Onerilen": (None if (y_suggested is None or pd.isna(y_suggested)) else float(y_suggested)),
                                    "CI_low": float(lo),
                                    "CI_high": float(hi),
                                }
                            ])
                            st.download_button(
                                "ğŸ“¥ Tahmin (CSV)", df_pred_row.to_csv(index=False), f"tahmin_{_strip_tr(str(sec_alan))}_{int(target_year)}.csv", "text/csv", key="tahmin_csv_single"
                            )

        # 6) KÃ¼meleme (opsiyonel)

    except Exception as ex:
        st.error(f"Beklenmeyen hata: {ex}")
else:
    st.info("Ã‡alÄ±ÅŸtÄ±rmak iÃ§in â€˜Ã‡alÄ±ÅŸtÄ±râ€™ dÃ¼ÄŸmesine basÄ±n veya One-Click modu aktifse ekran otomatik yenilenir.")